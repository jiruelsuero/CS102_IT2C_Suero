"","title","author","subject","abstract","date"
"1","chatuie: exploring chat-based unified information extraction using large language models","jun xu, mengshu sun, zhiqiang zhang, jun zhou","computation and language","recent advancements in large language models have shown impressive performance in general chat. however, their domain-specific capabilities, particularly in information extraction, have certain limitations. extracting structured information from natural language that deviates from known schemas or instructions has proven challenging for previous prompt-based methods. this motivated us to explore domain-specific modeling in chat-based language models as a solution for extracting structured information from natural language. in this paper, we present chatuie, an innovative unified information extraction framework built upon chatglm. simultaneously, reinforcement learning is employed to improve and align various tasks that involve confusing and limited samples. furthermore, we integrate generation constraints to address the issue of generating elements that are not present in the input. our experimental results demonstrate that chatuie can significantly improve the performance of information extraction with a slight decrease in chatting ability.",2024-03-08
"2","secure information embedding and extraction in forensic 3d fingerprinting","canran wang, jinwen wang, mi zhou, vinh pham, senyue hao, chao zhou, ning zhang, netanel raviv","cryptography and security","the prevalence of 3d printing poses a significant risk to public safety, as any individual with internet access and a commodity printer is able to produce untraceable firearms, keys, counterfeit products, etc. to aid government authorities in combating these new security threats, several approaches have been taken to tag 3d-prints with identifying information. known as fingerprints, this information is written into the object using various bit embedding techniques; examples include varying the height of the molten thermoplastic layers, and depositing metallic powder with different magnetic properties. yet, the practicality of theses techniques in real-world forensic settings is hindered by the adversarial nature of this problem. that is, the 3d-printing process is out of reach of any law enforcement agencies; it is the adversary who controls all aspects of printing and possesses the printed object. to combat these threats, law enforcement agencies can regulate the manufacturing of 3d printers, on which they may enforce a fingerprinting scheme, and collect adversarially tampered remains (e.g., fragments of a broken 3d-printed firearm) during forensic investigation. therefore, it is important to devise fingerprinting techniques so that the fingerprint could be extracted even if printing is carried out by the adversary. to this end, we present side (secure information embedding and extraction), a fingerprinting framework that tackles the adversarial nature of forensic fingerprinting in 3d prints by offering both secure information embedding and secure information extraction.",2024-03-07
"3","automating the information extraction from semi-structured interview transcripts","angelina parfenova","computation and language","this paper explores the development and application of an automated system designed to extract information from semi-structured interview transcripts. given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. our research investigates various topic modeling techniques and concludes that the best model for analyzing interview texts is a combination of bert embeddings and hdbscan clustering. we present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. this tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of topics revealed, thereby enhancing the depth of qualitative analysis.",2024-03-07
"4","quantum-enhanced joint estimation of phase and phase diffusion","jayanth jayakumar, monika e. mycroft, marco barbieri, magdalena stobi≈Ñska","quantum physics","accurate phase estimation in the presence of unknown phase diffusive noise is a crucial yet challenging task in noisy quantum metrology. this problem is particularly interesting due to the detrimental impact of the associated noise. here, we investigate the joint estimation of phase and phase diffusion using generalized holland-burnett states, known for their experimental accessibility. these states provide performance close to the optimal state in single-parameter phase estimation, even in the presence of photon losses. we adopt a twofold approach by analyzing the joint information extraction through the double homodyne measurement and the joint information availability across all probe states. through our analysis, we find that the highest sensitivities are obtained by using states created by directing all input photons into one port of a balanced beam splitter. furthermore, we infer that good levels of sensitivity persist even in the presence of moderate photon losses, illustrating the remarkable resilience of our probe states under lossy conditions.",2024-03-07
"5","sentiment-driven prediction of financial returns: a bayesian-enhanced finbert approach","raffaele giuseppe cestari, simone formentin","computational engineering, finance, and science","predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data. enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment. in this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the finbert large language model. by meticulously curating an optimal feature set through correlation analysis and employing bayesian-optimized recursive feature elimination for automatic feature selection, we surpass existing methodologies, achieving an f1-score exceeding 70% on the test set. this success translates into demonstrably higher cumulative profits during backtested trading. our investigation focuses on real-world spy etf data alongside corresponding tweets sourced from the stocktwits platform.",2024-03-07
"6","kernel correlation-dissimilarity for multiple kernel k-means clustering","rina su, yu guo, caiying wu, qiyu jin, tieyong zeng","machine learning","the main objective of the multiple kernel k-means (mkkm) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. to tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. by emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale. we assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art mkkm techniques.",2024-03-06
"7","mad libs are all you need: augmenting cross-domain document-level event argument data","joseph gatto, parker seegmiller, omar sharif, sarah m. preum","computation and language","document-level event argument extraction (doceae) is an extremely difficult information extraction problem -- with significant limitations in low-resource cross-domain settings. to address this problem, we introduce mad lib aug (mla), a novel generative doceae data augmentation framework. our approach leverages the intuition that mad libs, which are categorically masked documents used as a part of a popular game, can be generated and solved by llms to produce data for doceae. using mla, we achieve a 2.6-point average improvement in overall f1 score. moreover, this approach achieves a 3.9 and 5.2 point average increase in zero and few-shot event roles compared to augmentation-free baselines across all experiments.
to better facilitate analysis of cross-domain doceae, we additionally introduce a new metric, role-depth f1 (rdf1), which uses statistical depth to identify roles in the target domain which are semantic outliers with respect to roles observed in the source domain. our experiments show that mla augmentation can boost rdf1 performance by an average of 5.85 points compared to non-augmented datasets.",2024-03-05
"8","quantum mixed-state self-attention network","fu chen, qinglin zhao, li feng, chuangtao chen, yangbin lin, jianhong lin","quantum physics","the rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (nlp) tasks. quantum machine learning (qml) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. this paper introduces a novel quantum mixed-state attention network (qmsan), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling nlp tasks. qmsan model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. additionally, we propose an innovative quantum positional encoding scheme, implemented through fixed quantum gates within the quantum circuit, to enhance the model's accuracy. experimental validation on various datasets demonstrates that qmsan model outperforms existing quantum and classical models in text classification, achieving significant performance improvements. qmsan model not only significantly reduces the number of parameters but also exceeds classical self-attention networks in performance, showcasing its strong capability in data representation and information extraction. furthermore, our study investigates the model's robustness in different quantum noise environments, showing that qmsan possesses commendable robustness to low noise.",2024-03-05
"9","code-accord: a corpus of building regulatory data for rule generation towards automatic compliance checking","hansi hettiarachchi, amna dridi, mohamed medhat gaber, pouyan parsafard, nicoleta bocaneala, katja breitenfelder, gon√ßal costa, maria hedblom, mihaela juganaru-mathieu, thamer mecharnia, sumee park, he tan, abdel-rahman h. tawil, edlira vakaj","information retrieval","automatic compliance checking (acc) within the architecture, engineering, and construction (aec) sector necessitates automating the interpretation of building regulations to achieve its full potential. however, extracting information from textual rules to convert them to a machine-readable format has been a challenge due to the complexities associated with natural language and the limited resources that can support advanced machine-learning techniques. to address this challenge, we introduce code-accord, a unique dataset compiled under the eu horizon accord project. code-accord comprises 862 self-contained sentences extracted from the building regulations of england and finland. aligned with our core objective of facilitating information extraction from text for machine-readable rule generation, each sentence was annotated with entities and relations. entities represent specific components such as ""window"" and ""smoke detectors"", while relations denote semantic associations between these entities, collectively capturing the conveyed ideas in natural language. we manually annotated all the sentences using a group of 12 annotators. each sentence underwent annotations by multiple annotators and subsequently careful data curation to finalise annotations, ensuring their accuracy and reliability, thereby establishing the dataset as a solid ground truth. code-accord offers a rich resource for diverse machine learning and natural language processing (nlp) related tasks in acc, including text classification, entity recognition and relation extraction. to the best of our knowledge, this is the first entity and relation-annotated dataset in compliance checking, which is also publicly available.",2024-03-04
"10","representation learning on heterophilic graph with directional neighborhood attention","qincheng lu, jiaqi zhu, sitao luan, xiao-wen chang","machine learning","graph attention network (gat) is one of the most popular graph neural network (gnn) architecture, which employs the attention mechanism to learn edge weights and has demonstrated promising performance in various applications. however, since it only incorporates information from immediate neighborhood, it lacks the ability to capture long-range and global graph information, leading to unsatisfactory performance on some datasets, particularly on heterophilic graphs. to address this limitation, we propose the directional graph attention network (dgat) in this paper. dgat is able to combine the feature-based attention with the global directional information extracted from the graph topology. to this end, a new class of laplacian matrices is proposed which can provably reduce the diffusion distance between nodes. based on the new laplacian, topology-guided neighbour pruning and edge adding mechanisms are proposed to remove the noisy and capture the helpful long-range neighborhood information. besides, a global directional attention is designed to enable a topological-aware information propagation. the superiority of the proposed dgat over the baseline gat has also been verified through experiments on real-world benchmarks and synthetic data sets. it also outperforms the state-of-the-art (sota) models on 6 out of 7 real-world benchmark datasets.",2024-03-03
"11","a regularization-based transfer learning method for information extraction via instructed graph decoder","kedi chen, jie zhou, qin chen, shunyu liu, liang he","machine learning","information extraction (ie) aims to extract complex structured information from the text. numerous datasets have been constructed for various ie tasks, leading to time-consuming and labor-intensive data annotations. nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different ie tasks is not explicitly modeled. moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. in this study, we propose a regularization-based transfer learning method for ie (tie) via an instructed graph decoder. specifically, we first construct an instruction pool for datasets from all well-known ie tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. in this way, the common knowledge shared with existing datasets can be learned and transferred to a new dataset with new labels. furthermore, to alleviate the label inconsistency problem among various ie tasks, we introduce a task-specific regularization strategy, which does not update the gradients of two tasks with 'opposite direction'. we conduct extensive experiments on 12 datasets spanning four ie tasks, and the results demonstrate the great advantages of our proposed method",2024-03-01
"12","softtiger: a clinical foundation model for healthcare workflows","ye chen, igor couto, wei cai, cong fu, bruno dorneles","computation and language","we release and introduce softtiger, a clinical large language model (clam) designed as a foundation model for healthcare workflows. the narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. we address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. we collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. we then supervised fine-tuned a state-of-the-art llm using public and credentialed clinical data. the training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. moreover, we address, several modeling challenges in the healthcare context, e.g., extra long context window. our blind pairwise evaluation shows that softtiger outperforms other popular open-source models and gpt-3.5, comparable to gemini-pro, and only has a mild gap from gpt-4. we believe that llms may become a step-stone towards healthcare digitalization and democratization. therefore, we publicly release softtiger models at scales of 13 billion and 70 billion parameters, as well as datasets and code for our innovative scalable evaluation, hopefully, making a significant contribution to the healthcare industry.",2024-03-01
"13","iped: an implicit perspective for relational triple extraction based on diffusion model","jianli zhao, changhao xu, bin jiang","computation and language","relational triple extraction is a fundamental task in the field of information extraction, and a promising framework based on table filling has recently gained attention as a potential baseline for entity relation extraction. however, inherent shortcomings such as redundant information and incomplete triple recognition remain problematic. to address these challenges, we propose an implicit perspective for relational triple extraction based on diffusion model (iped), an innovative approach for extracting relational triples. our classifier-free solution adopts an implicit strategy using block coverage to complete the tables, avoiding the limitations of explicit tagging methods. additionally, we introduce a generative model structure, the block-denoising diffusion model, to collaborate with our implicit perspective and effectively circumvent redundant information disruptions. experimental results on two popular datasets demonstrate that iped achieves state-of-the-art performance while gaining superior inference speed and low computational complexity. to support future research, we have made our source code publicly available online.",2024-02-24
"14","enhancing steganographic text extraction: evaluating the impact of nlp models on accuracy and semantic coherence","mingyang li, maoqin yuan, luyao li, han pengsihua","computer vision and pattern recognition","this study discusses a new method combining image steganography technology with natural language processing (nlp) large models, aimed at improving the accuracy and robustness of extracting steganographic text. traditional least significant bit (lsb) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as chinese characters. to address this issue, this study proposes an innovative lsb-nlp hybrid framework. this framework integrates the advanced capabilities of nlp large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. experimental results show that the lsb-nlp hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling chinese characters. the findings of this study not only confirm the effectiveness of combining image steganography technology and nlp large models but also propose new ideas for research and application in the field of information hiding. the successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.",2024-02-29
"15","hierarchical multimodal pre-training for visually rich webpage understanding","hongshen xu, lu chen, zihan zhao, da ma, ruisheng cao, zichen zhu, kai yu","computation and language","the growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, pdfs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. in this paper, we introduce weblm, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of html in webpages. instead of processing document images as unified natural images, weblm integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. empirical results demonstrate that the pre-trained weblm significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. the pre-trained models and code are available at this https url.",2024-02-28
"16","on the use of silver standard data for zero-shot classification tasks in information extraction","jianwei wang, tianyin wang, ziqian zeng","computation and language","the superior performance of supervised classification methods in the information extraction (ie) area heavily relies on a large amount of gold standard data. recent zero-shot classification methods converted the task to other nlp tasks (e.g., textual entailment) and used off-the-shelf models of these nlp tasks to directly perform inference on the test data without using a large amount of ie annotation data. a potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other nlp tasks. however, there is no further investigation into the use of these data. in this paper, we propose a new framework, clean-lave, which aims to utilize silver standard data to enhance the zero-shot performance. clean-lave includes four phases: (1) obtaining silver data; (2) identifying relatively clean data from silver data; (3) finetuning the off-the-shelf model using clean data; (4) inference on the test data. the experimental results show that clean-lave can outperform the baseline by 5% and 6% on tacred and wiki80 dataset in the zero-shot relation classification task, and by 3%-7% on smile (korean and polish) in the zero-shot cross-lingual relation classification task, and by 8% on ace05-e+ in the zero-shot event argument classification task. the code is share in this https url.",2024-02-28
"17","mixer is more than just a model","qingfeng ji, yuxin wang, letong sun","machine learning","recently, mlp structures have regained popularity, with mlp-mixer standing out as a prominent example. in the field of computer vision, mlp-mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. indeed, mixer represents a paradigm for information extraction that amalgamates channel and token information. the essence of mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of ""mixing"" in the realm of neural network architectures. beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. this study focuses on the domain of audio recognition, introducing a novel model named audio spectrogram mixer with roll-time and hermit fft (asm-rh) that incorporates insights from both time and frequency domains. experimental results demonstrate that asm-rh is particularly well-suited for audio data and yields promising outcomes across multiple classification tasks. the models and optimal weights files will be published.",2024-02-28
"18","deep learning based named entity recognition models for recipes","mansi goel, ayush agarwal, shubham agrawal, janak kapuriya, akhil vamshi konam, rishabh gupta, shrey rastogi, niharika, ganesh bagler","computation and language","food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. recipes are cultural capsules transmitted across generations via unstructured text. automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. simultaneously, we systematically cleaned and analyzed ingredient phrases from recipedb, the gold-standard recipe data repository, and annotated them using the stanford ner. based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity to create the machine-annotated dataset. a thorough investigation of ner approaches on these three datasets involving statistical, fine-tuning of deep learning-based language models and few-shot prompting on large language models (llms) provides deep insights. we conclude that few-shot prompting on llms has abysmal performance, whereas the fine-tuned spacy-transformer emerges as the best model with macro-f1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.",2024-02-27
"19","domain embeddings for generating complex descriptions of concepts in italian language","alessandro maisto","computation and language","in this work, we propose a distributional semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries, designed to address the challenge of bridging the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions offered by general semantics theory. recently, many researchers have concentrated on the nexus between embeddings and a comprehensive theory of semantics and meaning. this often involves decoding the representation of word meanings in distributional models into a set of discrete, manually constructed properties such as semantic primitives or features, using neural decoding techniques. our approach introduces an alternative strategy grounded in linguistic data. we have developed a collection of domain-specific co-occurrence matrices, derived from two sources: a classification of italian nouns categorized into 4 semantic traits and 20 concrete noun sub-categories, and a list of italian verbs classified according to their semantic classes. in these matrices, the co-occurrence values for each word are calculated exclusively with a defined set of words pertinent to a particular lexical domain. the resource comprises 21 domain-specific matrices, one comprehensive matrix, and a graphical user interface. our model facilitates the generation of reasoned semantic descriptions of concepts by selecting matrices directly associated with concrete conceptual knowledge, such as a matrix based on location nouns and the concept of animal habitats. we assessed the utility of the resource through two experiments, achieving promising outcomes in both: the automatic classification of animal nouns and the extraction of animal features.",2024-02-26
"20","dcvsmnet: double cost volume stereo matching network","mahmoud tahmasebi, saif huq, kevin meehan, marion mcafee","computer vision and pattern recognition","we introduce double cost volume stereo matching network(dcvsmnet) which is a novel architecture characterised by by two small upper (group-wise) and lower (norm correlation) cost volumes. each cost volume is processed separately, and a coupling module is proposed to fuse the geometry information extracted from the upper and lower cost volumes. dcvsmnet is a fast stereo matching network with a 67 ms inference time and strong generalization ability which can produce competitive results compared to state-of-the-art methods. the results on several bench mark datasets show that dcvsmnet achieves better accuracy than methods such as cgi-stereo and bgnet at the cost of greater inference time.",2024-02-26
"21","a joint communication and computation design for probabilistic semantic communications","zhouxiang zhao, zhaohui yang, mingzhe chen, zhaoyang zhang, h. vincent poor","information theory","in this paper, the problem of joint transmission and computation resource allocation for a multi-user probabilistic semantic communication (psc) network is investigated. in the considered model, users employ semantic information extraction techniques to compress their large-sized data before transmitting them to a multi-antenna base station (bs). our model represents large-sized data through substantial knowledge graphs, utilizing shared probability graphs between the users and the bs for efficient semantic compression. the resource allocation problem is formulated as an optimization problem with the objective of maximizing the sum of equivalent rate of all users, considering total power budget and semantic resource limit constraints. the computation load considered in the psc network is formulated as a non-smooth piecewise function with respect to the semantic compression ratio. to tackle this non-convex non-smooth optimization challenge, a three-stage algorithm is proposed where the solutions for the receive beamforming matrix of the bs, transmit power of each user, and semantic compression ratio of each user are obtained stage by stage. numerical results validate the effectiveness of our proposed scheme.",2024-02-26
"22","iepile: unearthing large-scale schema-based information extraction corpus","honghao gui, hongbin ye, lin yuan, ningyu zhang, mengshu sun, lei liang, huajun chen","computation and language","large language models (llms) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in information extraction (ie). note that high-quality instruction data is the vital key for enhancing the specific capabilities of llms, while current ie datasets tend to be small in scale, fragmented, and lack standardized schema. to this end, we introduce iepile, a comprehensive bilingual (english and chinese) ie instruction corpus, which contains approximately 0.32b tokens. we construct iepile by collecting and cleaning 33 existing ie datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. experimental results on llama and baichuan demonstrate that using iepile can enhance the performance of llms for ie, especially the zero-shot generalization. we open-source the resource and pre-trained models, hoping to provide valuable support to the nlp community.",2024-02-22
"23","llm-da: data augmentation via large language models for few-shot named entity recognition","junjie ye, nuo xu, yikun wang, jie zhou, qi zhang, tao gui, xuanjing huang","computation and language","despite the impressive capabilities of large language models (llms), their performance on information extraction tasks is still not entirely satisfactory. however, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. in this paper, we propose $llm-da$, a novel data augmentation technique based on llms for the few-shot ner task. to overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in llm-generated text, we leverage the distinctive characteristics of the ner task by augmenting the original data at both the contextual and entity levels. our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. extensive experiments demonstrate the effectiveness of our approach in enhancing ner model performance with limited data. furthermore, additional analyses provide further evidence supporting the assertion that the quality of the data we generate surpasses that of other existing methods.",2024-02-22
"24","hint: high-quality inpainting transformer with mask-aware encoding and enhanced attention","shuang chen, amir atapour-abarghouei, hubert p. h. shum","computer vision and pattern recognition","existing image inpainting methods leverage convolution-based downsampling approaches to reduce spatial dimensions. this may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. recent advances in self-attention mechanisms within transformers have led to significant improvements in many computer vision tasks including inpainting. however, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. in this paper, we propose an end-to-end high-quality inpainting transformer, abbreviated as hint, which consists of a novel mask-aware pixel-shuffle downsampling module (mpd) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. moreover, we propose a spatially-activated channel attention layer (scal), an efficient self-attention mechanism interpreting spatial awareness to model the corrupted image at multiple scales. to further enhance the effectiveness of scal, motivated by recent advanced in speech recognition, we introduce a sandwich structure that places feed-forward networks before and after the scal module. we demonstrate the superior performance of hint compared to contemporary state-of-the-art models on four datasets, celeba, celeba-hq, places2, and dunhuang.",2024-02-22
"25","combining language and graph models for semi-structured information extraction on the web","zhi hong, kyle chard, ian foster","information retrieval","relation extraction is an efficient way of mining the extraordinary wealth of human knowledge on the web. existing methods rely on domain-specific training data or produce noisy outputs. we focus here on extracting targeted relations from semi-structured web pages given only a short description of the relation. we present graphscholarbert, an open-domain information extraction method based on a joint graph and language model structure. graphscholarbert can generalize to previously unseen domains without additional data or training and produces only clean extraction results matched to the search keyword. experiments show that graphscholarbert can improve extraction f1 scores by as much as 34.8\% compared to previous work in a zero-shot domain and zero-shot website setting.",2024-02-21
"26","a simple but effective approach to improve structured language model output for information extraction","yinghao li, rampi ramprasad, chao zhang","computation and language","large language models (llms) have demonstrated impressive abilities in generating unstructured natural language according to instructions. however, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (ner) or relation extraction (re). to address this issue, this paper introduces an efficient method, g&o, to enhance their structured text generation capabilities. it breaks the generation into a two-step pipeline: initially, llms generate answers in natural language as intermediate responses. subsequently, llms are asked to organize the output into the desired structure, using the intermediate responses as context. g&o effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. tested on zero-shot ner and re, the results indicate a significant improvement in llm performance with minimal additional efforts. this straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate llm capabilities in various structured text generation tasks.",2024-02-20
"27","creating a fine grained entity type taxonomy using llms","michael gunn, dohyun park, nidhish kamath","computation and language","in this study, we investigate the potential of gpt-4 and its advanced iteration, gpt-4 turbo, in autonomously developing a detailed entity type taxonomy. our objective is to construct a comprehensive taxonomy, starting from a broad classification of entity types - including objects, time, locations, organizations, events, actions, and subjects - similar to existing manually curated taxonomies. this classification is then progressively refined through iterative prompting techniques, leveraging gpt-4's internal knowledge base. the result is an extensive taxonomy comprising over 5000 nuanced entity types, which demonstrates remarkable quality upon subjective evaluation.
we employed a straightforward yet effective prompting strategy, enabling the taxonomy to be dynamically expanded. the practical applications of this detailed taxonomy are diverse and significant. it facilitates the creation of new, more intricate branches through pattern-based combinations and notably enhances information extraction tasks, such as relation extraction and event argument extraction. our methodology not only introduces an innovative approach to taxonomy creation but also opens new avenues for applying such taxonomies in various computational linguistics and ai-related fields.",2024-02-19
"28","codeart: better code models by attention regularization when symbols are lacking","zian su, xiangzhe xu, ziyang huang, zhuo zhang, yapeng ye, jianjun huang, xiangyu zhang","software engineering","transformer based code models have impressive performance in many software engineering tasks. however, their effectiveness degrades when symbols are missing or not informative. the reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. we propose a new method to pre-train general code models when symbols are lacking. we observe that in such cases, programs degenerate to something written in a very primitive language. we hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). we then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. in the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more important compared to others. to realize the idea, we enhance the vanilla tokenization and model architecture of a bert model, construct and utilize attention masks, and introduce a new pre-training algorithm. we pre-train this bert-like model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. we apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. our pre-trained model can improve the sotas in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. it also substantially outperforms other general pre-training techniques of code understanding models.",2024-02-19
"29","c-icl: contrastive in-context learning for information extraction","ying mo, jian yang, jiahao liu, shun zhang, jingang wang, zhoujun li","computation and language","recently, there has been increasing interest in exploring the capabilities of advanced large language models (llms) in the field of information extraction (ie), specifically focusing on tasks related to named entity recognition (ner) and relation extraction (re). although researchers are exploring the use of few-shot information extraction through in-context learning with llms, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. in this paper, we present c-icl, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. this approach enhances the ability of llms to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. this method allows for the identification and correction of potential interface errors. specifically, our proposed method taps into the inherent contextual information and valuable information in hard negative samples and the nearest positive neighbors to the test and then applies the in-context learning demonstrations based on llms. our experiments on various datasets indicate that c-icl outperforms previous few-shot in-context learning methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. these improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.",2024-02-17
"30","a question answering based pipeline for comprehensive chinese ehr information extraction","huaiyuan ying, sheng yu","computation and language","electronic health records (ehrs) hold significant value for research and applications. as a new way of information extraction, question answering (qa) can extract more flexible information than conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. in this paper, we propose a novel approach that automatically generates training data for transfer learning of qa models. our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive qa frameworks, including cases with discontinuous answers and many-to-one relationships. the obtained qa model exhibits excellent performance on subtasks of information extraction in ehrs, and it can effectively handle few-shot or zero-shot settings involving yes-no questions. case studies and ablation studies demonstrate the necessity of each component in our design, and the resulting model is deemed suitable for practical use.",2024-02-17
"31","construction of a syntactic analysis map for yi shui school through text mining and natural language processing research","hanqing zhao, yuehan li","computation and language","entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis. most of the information of the yishui school of traditional chinese medicine (tcm) is stored in the form of unstructured classical chinese text. the key information extraction of tcm texts plays an important role in mining and studying the academic schools of tcm. in order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional chinese medicine texts, and uses the common weighting technology of tf-idf information retrieval and data mining to extract important key entity information in different ancient books. the dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the knowledge graph of yishui school and the use of artificial intelligence methods to carry out the research of tcm academic schools.",2024-02-16
"32","fine tuning named entity extraction models for the fantasy domain","aravinth sivaganeshan, nisansa de silva","computation and language","named entity recognition (ner) is a sequence classification natural language processing task where entities are identified in the text and classified into predefined categories. it acts as a foundation for most information extraction systems. dungeons and dragons (d&d) is an open-ended tabletop fantasy game with its own diverse lore. dnd entities are domain-specific and are thus unrecognizable by even the state-of-the-art off-the-shelf ner systems as the ner systems are trained on general data for pre-defined categories such as: person (pers), location (loc), organization (org), and miscellaneous (misc). for meaningful extraction of information from fantasy text, the entities need to be classified into domain-specific entity categories as well as the models be fine-tuned on a domain-relevant corpus. this work uses available lore of monsters in the d&d domain to fine-tune trankit, which is a prolific ner framework that uses a pre-trained model for ner. upon this training, the system acquires the ability to extract monster names from relevant domain documents under a novel ner tag. this work compares the accuracy of the monster name identification against; the zero-shot trankit model and two flair models. the fine-tuned trankit model achieves an 87.86% f1 score surpassing all the other considered models.",2024-02-16
"33","modeling the impact of timeline algorithms on opinion dynamics using low-rank updates","tianyi zhou, stefan neumann, kiran garimella, aristides gionis","social and information networks","timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. we make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. specifically, we show how the popular friedkin--johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. we use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users' timeline compositions by strengthening some topics of discussion and penalizing some others. we present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\varepsilon)$-approximate solution in time $\tilde{o}(m\sqrt{n} \lg(1/\varepsilon))$, where $m$ is the number of edges in the graph and $n$ is the number of vertices. we also present an algorithm that provably computes an $\varepsilon$-approximation of our model in near-linear time. we evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. finally, we release an anonymized graph dataset with ground-truth opinions and more than 27\,000 nodes (the previously largest publicly available dataset contains less than 550 nodes).",2024-02-15
"34","massively multi-cultural knowledge acquisition & lm benchmarking","yi fung, ruining zhao, jae doo, chenkai sun, heng ji","computation and language","pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. specifically, our method strategically navigates from densely informative wikipedia documents on cultural topics to an extensive network of linked pages. leveraging this valuable source of data collection, we construct the cultureatlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in ai, to promote a more inclusive and balanced representation of global cultures in the digital domain.",2024-02-14
"35","magic-me: identity-specific video customized diffusion","ze ma, daquan zhou, chun-hsiao yeh, xue-she wang, xiuyu li, huanrui yang, zhen dong, kurt keutzer, jiashi feng","computer vision and pattern recognition","creating content for a specific identity (id) has shown significant interest in the field of generative models. in the field of text-to-image generation (t2i), subject-driven content generation has achieved great progress with the id in the images controllable. however, extending it to video generation is not well explored. in this work, we propose a simple yet effective subject identity controllable video generation framework, termed video custom diffusion (vcd). with a specified subject id defined by a few images, vcd reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. to achieve this, we propose three novel components that are essential for high-quality id preservation: 1) an id module trained with the cropped identity by prompt-to-segmentation to disentangle the id information and the background noise for more accurate id token learning; 2) a text-to-video (t2v) vcd module with 3d gaussian noise prior for better inter-frame consistency and 3) video-to-video (v2v) face vcd and tiled vcd modules to deblur the face and upscale the video for higher resolution.
despite its simplicity, we conducted extensive experiments to verify that vcd is able to generate stable and high-quality videos with better id over the selected strong baselines. besides, due to the transferability of the id module, vcd is also working well with finetuned text-to-image models available publically, further improving its usability. the codes are available at this https url.",2024-02-14
"36","structured language generation model for robust structure prediction","minho lee, junghyun min, woochul lee, yeonsoo lee","computation and language","previous work in structured prediction (e.g. ner, information extraction) using single model make use of explicit dataset information, which helps boost in-distribution performance but is orthogonal to robust generalization in real-world situations. to overcome this limitation, we propose the structured language generation model (slgm), a framework that reduces sequence-to-sequence problems to classification problems via methodologies in loss calibration and decoding method. our experimental results show that slgm is able to maintain performance without explicit dataset information, follow and potentially replace dataset-specific fine-tuning.",2024-02-14
"37","befunet: a hybrid cnn-transformer architecture for precise medical image segmentation","omid nejati manzari, javad mirzapour kaleybar, hooman saadat, shahin maleki","computer vision and pattern recognition","the accurate segmentation of medical images is critical for various healthcare applications. convolutional neural networks (cnns), especially fully convolutional networks (fcns) like u-net, have shown remarkable success in medical image segmentation tasks. however, they have limitations in capturing global context and long-range relations, especially for objects with significant variations in shape, scale, and texture. while transformers have achieved state-of-the-art results in natural language processing and image recognition, they face challenges in medical image segmentation due to image locality and translational invariance issues. to address these challenges, this paper proposes an innovative u-shaped network called befunet, which enhances the fusion of body and edge information for precise medical image segmentation. the befunet comprises three main modules, including a novel local cross-attention feature (lcaf) fusion module, a novel double-level fusion (dlf) module, and dual-branch encoder. the dual-branch encoder consists of an edge encoder and a body encoder. the edge encoder employs pdc blocks for effective edge information extraction, while the body encoder uses the swin transformer to capture semantic information with global attention. the lcaf module efficiently fuses edge and body features by selectively performing local cross-attention on features that are spatially close between the two modalities. this local approach significantly reduces computational complexity compared to global cross-attention while ensuring accurate feature matching. befunet demonstrates superior performance over existing methods across various evaluation metrics on medical image segmentation datasets.",2024-02-13
"38","punctuation restoration improves structure understanding without supervision","junghyun min, minho lee, woochul lee, yeonsoo lee","computation and language","unsupervised learning objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from natural language understanding to conversational tasks. however, despite impressive generative capabilities of recent large language models, their abilities to capture syntactic or semantic structure within text lag behind. we hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. we show that punctuation restoration as a learning objective improves in- and out-of-distribution performance on structure-related tasks like named entity recognition, open information extraction, chunking, and part-of-speech tagging. punctuation restoration is an effective learning objective that can improve structure understanding and yield a more robust structure-aware representations of natural language.",2024-02-13
"39","structural descriptors and information extraction from x-ray spectra of liquids","e. a. eronen, a. vladyka, ch. j. sahle, j. niskanen","chemical physics","machine learning can reveal new insights into x-ray spectroscopy of liquids when the local atomistic environment is presented to the model in a suitable way. many unique structural descriptor families have been developed for this purpose. we benchmark the performance of six different descriptor types using a computational data set of 24200 sulfur k\b{eta} x-ray emission spectra of aqueous sulfuric acid simulated at six different concentrations. we train a feed-forward neural network to predict the spectra from the corresponding descriptor vectors and find that the local many-body tensor representation, smooth overlap of atomic positions and atom-centered symmetry functions excel in this comparison. we found a similar hierarchy when applying the emulator-based component analysis to identify the spectrally relevant structural characteristics. the spectra were dominantly dependent on the concentration of the system, whereas adding the second most significant degree of freedom in the decomposition allowed for distinction of the protonation state of the acid molecule.",2024-02-13
"40","cpsdbench: a large language model evaluation benchmark and baseline for chinese public security domain","xin tong, bo jin, zhi lin, binjun wang, ting yu, qiang cheng","artificial intelligence","large language models (llms) have demonstrated significant potential and effectiveness across multiple application domains. to assess the performance of mainstream llms in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the chinese public security domain--cpsdbench. cpsdbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of llms across four key dimensions: text classification, information extraction, question answering, and text generation. furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of llms in executing tasks related to public security. through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized llm models targeted at applications in this field.",2024-02-11
"41","nlp for knowledge discovery and information extraction from energetics corpora","francis g. vangessel, efrem perry, salil mohan, oliver m. barham, mark cavolowsky","computation and language","we present a demonstration of the utility of nlp for aiding research into energetic materials and associated systems. the nlp method enables machine understanding of textual data, offering an automated route to knowledge discovery and information extraction from energetics text. we apply three established unsupervised nlp models: latent dirichlet allocation, word2vec, and the transformer to a large curated dataset of energetics-related scientific articles. we demonstrate that each nlp algorithm is capable of identifying energetic topics and concepts, generating a language model which aligns with subject matter expert knowledge. furthermore, we present a document classification pipeline for energetics text. our classification pipeline achieves 59-76\% accuracy depending on the nlp model used, with the highest performing transformer model rivaling inter-annotator agreement metrics. the nlp approaches studied in this work can identify concepts germane to energetics and therefore hold promise as a tool for accelerating energetics research efforts and energetics material development.",2024-02-10
"42","resumeflow: an llm-facilitated pipeline for personalized resume generation and refinement","saurabh bhausaheb zinjad, amrita bhattacharjee, amey bhilegaonkar, huan liu","computation and language","crafting the ideal, job-specific resume is a challenging task for many job applicants, especially for early-career applicants. while it is highly recommended that applicants tailor their resume to the specific role they are applying for, manually tailoring resumes to job descriptions and role-specific requirements is often (1) extremely time-consuming, and (2) prone to human errors. furthermore, performing such a tailoring step at scale while applying to several roles may result in a lack of quality of the edited resumes. to tackle this problem, in this demo paper, we propose resumeflow: a large language model (llm) aided tool that enables an end user to simply provide their detailed resume and the desired job posting, and obtain a personalized resume specifically tailored to that specific job posting in the matter of a few seconds. our proposed pipeline leverages the language understanding and information extraction capabilities of state-of-the-art llms such as openai's gpt-4 and google's gemini, in order to (1) extract details from a job description, (2) extract role-specific details from the user-provided resume, and then (3) use these to refine and generate a role-specific resume for the user. our easy-to-use tool leverages the user-chosen llm in a completely off-the-shelf manner, thus requiring no fine-tuning. we demonstrate the effectiveness of our tool via a video demo and propose novel task-specific evaluation metrics to control for alignment and hallucination. our tool is available at https://job-aligned-resume.streamlit.app.",2024-02-09
"43","selective forgetting: advancing machine unlearning techniques and evaluation in language models","lingzhi wang, xingshan zeng, jinsong guo, kam-fai wong, georg gottlob","computation and language","the aim of this study is to investigate machine unlearning (mu), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. here, a novel approach is introduced to achieve precise and selective forgetting within language models. unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. furthermore, two innovative evaluation metrics are proposed: sensitive information extraction likelihood (s-el) and sensitive information memory accuracy (s-ma), designed to gauge the effectiveness of sensitive information elimination. to reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. the online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline annotation entails a robust two-stage process based on large language models (llms).",2024-02-08
"44","are llms ready for real-world materials discovery?","santiago miret, n m anoop krishnan","materials science","large language models (llms) create exciting possibilities for powerful language processing tools to accelerate research in materials science. while llms have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. in this position paper, we show relevant failure cases of llms in materials science that reveal current limitations of llms related to comprehending and reasoning over complex, interconnected materials science knowledge. given those shortcomings, we outline a framework for developing materials science llms (matsci-llms) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. the path to attaining performant matsci-llms rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. as such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge. finally, we outline a roadmap for applying future matsci-llms for real-world materials discovery via: 1. automated knowledge base generation; 2. automated in-silico material design; and 3. matsci-llm integrated self-driving materials laboratories.",2024-02-07
"45","back action suppression for levitated dipolar scatterers","yannick weiser, tommaso faorlin, lorenz panzl, thomas lafenthaler, lorenzo dania, dmitry s. bykov, thomas monz, rainer blatt, giovanni cerchiari","quantum physics","levitated dipolar scatterers exhibit exceptional performance as optomechanical systems for observing quantum mechanics at the mesoscopic scale. however, their tendency to scatter light in almost any direction poses experimental challenges, in particular limiting light collection efficiencies and, consequently, the information extractable from the system. in this article, we present a setup designed to enhance the information gleaned from optomechanical measurements by constraining the back action to a specific spatial direction. this approach facilitates achieving heisenberg-limited detection at any given numerical aperture. the setup consists of a hollow hemispherical mirror that controls the light scattered by the dipolar emitter, particularly at high scattering angles, thereby focusing the obtained information. this mirror is compatible with existing setups commonly employed in levitated optomechanics, including confocal lenses and optical resonators.",2024-02-07
"46","reliable ligand discrimination in stochastic multistep kinetic proofreading: first passage time vs. product counting strategies","xiangting li, tom chou","molecular networks","cellular signaling, crucial for biological processes like immune response and homeostasis, relies on specificity and fidelity in signal transduction to accurately respond to stimuli amidst biological noise. kinetic proofreading (kpr) is a key mechanism enhancing signaling specificity through time-delayed steps, although its effectiveness is debated due to intrinsic noise potentially reducing signal fidelity. in this study, we reformulate the theory of kinetic proofreading (kpr) by convolving multiple intermediate states into a single state and then define an overall ""processing"" time required to traverse these states. this simplification allows us to succinctly describe kinetic proofreading in terms of a single waiting time parameter, facilitating a more direct evaluation and comparison of kpr performance across different biological contexts such as dna replication and t cell receptor (tcr) signaling. we find that loss of fidelity for longer proofreading steps relies on the specific strategy of information extraction and show that in the first-passage time (fpt) discrimination strategy, longer proofreading steps can exponentially improve the accuracy of kpr at the cost of speed. thus, kpr can still be an effective discrimination mechanism in the high noise regime. however, in a product concentration-based discrimination strategy, longer proofreading steps do not necessarily lead to an increase in performance. however, by introducing activation thresholds on product concentrations, can we decompose the product-based strategy into a series of fpt based strategies to better resolve the subtleties of kpr-mediated product discrimination. our findings underscore the importance of understanding kpr in the context of how information is extracted and processed in the cell.",2024-02-07
"47","structured entity extraction using large language models","haolun wu, ye yuan, liana mikaelyan, alexander meulemans, xue liu, james hensman, bhaskar mitra","computation and language","recent advances in machine learning have significantly impacted the field of information extraction, with large language models (llms) playing a pivotal role in extracting structured information from unstructured text. this paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. we contribute to the field by first introducing and formalizing the task of structured entity extraction (see), followed by proposing approximate entity set overlap (aesop) metric designed to appropriately assess model performance on this task. later, we propose a new model that harnesses the power of llms for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.",2024-02-06
"48","anls* -- a universal document processing metric for generative large language models","david peer, philemon sch√∂pf, volckmar nebendahl, alexander rietzler, sebastian stabinger","computation and language","traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. these models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the f1 score. however, recent advancements in generative large language models (gllms) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. however, evaluating gllms presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by gllms. this paper introduces a new metric for generative models called anls* for evaluating a wide variety of tasks, including information extraction and classification tasks. the anls* metric extends existing anls metrics as a drop-in-replacement and is still compatible with previously reported anls scores. an evaluation of 7 different datasets and 3 different gllms using the anls* metric is also provided, demonstrating the importance of the proposed metric. we also benchmark a novel approach to generate prompts for documents, called sft, against other prompting techniques such as latin. in 15 out of 21 cases, sft outperforms other techniques and improves the state-of-the-art, sometimes by as much as 15 percentage points.
sources are available at this https url",2024-02-06
"49","retrieval augmented cross-modal tag recommendation in software q&a sites","sijin lu, pengyu xu, bing liu, hongjian sun, liping jing, jian yu","information retrieval","posts in software q\&a sites often consist of three main parts: title, description and code, which are interconnected and jointly describe the question. existing tag recommendation methods often treat different modalities as a whole or inadequately consider the interaction between different modalities. additionally, they focus on extracting information directly from the post itself, neglecting the information from external knowledge sources. therefore, we propose a retrieval augmented cross-modal (racm) tag recommendation model in software q\&a sites. specifically, we first use the input post as a query and enhance the representation of different modalities by retrieving information from external knowledge sources. for the retrieval-augmented representations, we employ a cross-modal context-aware attention to leverage the main modality description for targeted feature extraction across the submodalities title and code. in the fusion process, a gate mechanism is employed to achieve fine-grained feature selection, controlling the amount of information extracted from the submodalities. finally, the fused information is used for tag recommendation. experimental results on three real-world datasets demonstrate that our model outperforms the state-of-the-art counterparts.",2024-02-06
"50","detection of tortured phrases in scientific literature","el√©na martel (sigma, lig), martin lentschat (sigma, getalp), cyril labb√© (lig, sigma )","information retrieval","this paper presents various automatic detection methods to extract so called tortured phrases from scientific papers. these tortured phrases, e.g. flag to clamor instead of signal to noise, are the results of paraphrasing tools used to escape plagiarism detection. we built a dataset and evaluated several strategies to flag previously undocumented tortured phrases. the proposed and tested methods are based on language models and either on embeddings similarities or on predictions of masked token. we found that an approach using token prediction and that propagates the scores to the chunk level gives the best results. with a recall value of .87 and a precision value of .61, it could retrieve new tortured phrases to be submitted to domain experts for validation.",2024-02-02
"51","nanoner: named entity recognition for nanobiology using experts' knowledge and distant supervision","martin lentschat (sigma, getalp), cyril labb√© (lig, sigma), ran cheng (lig, sigma)","information retrieval","here we present the training and evaluation of nanoner, a named entity recognition (ner) model for nanobiology. ner consists in the identification of specific entities in spans of unstructured texts and is often a primary task in natural language processing (nlp) and information extraction. the aim of our model is to recognise entities previously identified by domain experts as constituting the essential knowledge of the domain. relying on ontologies, which provide us with a domain vocabulary and taxonomy, we implemented an iterative process enabling experts to determine the entities relevant to the domain at hand. we then delve into the potential of distant supervision learning in ner, supporting how this method can increase the quantity of annotated data with minimal additional manpower. on our full corpus of 728 full-text nanobiology articles, containing more than 120k entity occurrences, nanoner obtained a f1-score of 0.98 on the recognition of previously known entities. our model also demonstrated its ability to discover new entities in the text, with precision scores ranging from 0.77 to 0.81. ablation experiments further confirmed this and allowed us to assess the dependency of our approach on the external resources. it highlighted the dependency of the approach to the resource, while also confirming its ability to rediscover up to 30% of the ablated terms. this paper details the methodology employed, experimental design, and key findings, providing valuable insights and directions for future related researches on ner in specialized domain. furthermore, since our approach require minimal manpower , we believe that it can be generalized to other specialized fields.",2024-01-30
"52","rethinking the evaluation of pre-trained text-and-layout models from an entity-centric perspective","chong zhang, yixi zhao, chenshu yuan, yi tu, ya guo, qi zhang","computation and language","recently developed pre-trained text-and-layout models (ptlms) have shown remarkable success in multiple information extraction tasks on visually-rich documents. however, the prevailing evaluation pipeline may not be sufficiently robust for assessing the information extraction ability of ptlms, due to inadequate annotations within the benchmarks. therefore, we claim the necessary standards for an ideal benchmark to evaluate the information extraction ability of ptlms. we then introduce ec-funsd, an entity-centric benckmark designed for the evaluation of semantic entity recognition and entity linking on visually-rich documents. this dataset contains diverse formats of document layouts and annotations of semantic-driven entities and their relations. moreover, this dataset disentangles the falsely coupled annotation of segment and entity that arises from the block-level annotation of funsd. experiment results demonstrate that state-of-the-art ptlms exhibit overfitting tendencies on the prevailing benchmarks, as their performance sharply decrease when the dataset bias is removed.",2024-02-04
"53","different tastes of entities: investigating human label variation in named entity annotations","siyao peng, zihang sun, sebastian loftus, barbara plank","computation and language","named entity recognition (ner) is a key information extraction task with a long-standing tradition. while recent studies address and aim to correct annotation errors via re-labeling efforts, little is known about the sources of human label variation, such as text ambiguity, annotation error, or guideline divergence. this is especially the case for high-quality datasets and beyond english conll03. this paper studies disagreements in expert-annotated named entity datasets for three languages: english, danish, and bavarian. we show that text ambiguity and artificial guideline changes are dominant factors for diverse annotations among high-quality revisions. we survey student annotations on a subset of difficult entities and substantiate the feasibility and necessity of manifold annotations for understanding named entity ambiguities from a distributional perspective.",2024-02-02
"54","bridging semantics for automated web form testing","parsa alian, noor nashid, mobina shahbandeh, ali mesbah","software engineering","automated test generation for web forms has been a longstanding challenge, exacerbated by the intrinsic human-centric design of forms and their complex, device-agnostic structures. we introduce an innovative approach, called formnexus, for automated web form test generation, which emphasizes deriving semantic insights from individual form elements and relations among them, utilizing textual content, dom tree structures, and visual proximity. the insights gathered are transformed into a new conceptual graph, the form entity relation graph (ferg), which offers machine-friendly semantic information extraction. leveraging llms, formnexus adopts a feedback-driven mechanism for generating and refining input constraints based on real-time form submission responses. the culmination of this approach is a robust set of test cases, each produced by methodically invalidating constraints, ensuring comprehensive testing scenarios for web forms. this work bridges the existing gap in automated web form testing by intertwining the capabilities of llms with advanced semantic inference methods. our evaluation demonstrates that formnexus combined with gpt-4 achieves 89% coverage in form submission states. this outcome significantly outstrips the performance of the best baseline model by a margin of 25%.",2024-02-01
"55","lf tracy: a unified single-pipeline approach for salient object detection in light field cameras","fei teng, jiaming zhang, jiawei liu, kunyu peng, xina cheng, zhiyong li, kailun yang","computer vision and pattern recognition","leveraging the rich information extracted from light field (lf) cameras is instrumental for dense prediction tasks. however, adapting light field data to enhance salient object detection (sod) still follows the traditional rgb methods and remains under-explored in the community. previous approaches predominantly employ a custom two-stream design to discover the implicit angular feature within light field cameras, leading to significant information isolation between different lf representations. in this study, we propose an efficient paradigm (lf tracy) to address this limitation. we eschew the conventional specialized fusion and decoder architecture for a dual-stream backbone in favor of a unified, single-pipeline approach. this comprises firstly a simple yet effective data augmentation strategy called mixld to bridge the connection of spatial, depth, and implicit angular information under different lf representations. a highly efficient information aggregation (ia) module is then introduced to boost asymmetric feature-wise information fusion. owing to this innovative approach, our model surpasses the existing state-of-the-art methods, particularly demonstrating a 23% improvement over previous results on the latest large-scale pku dataset. by utilizing only 28.9m parameters, the model achieves a 10% increase in accuracy with 3m additional parameters compared to its backbone using rgb images and an 86% rise to its backbone using lf images. the source code will be made publicly available at this https url.",2024-01-30
"56","autoie: an automated framework for information extraction from scientific literature","yangyang liu, shoubin li","information retrieval","in the rapidly evolving field of scientific research, efficiently extracting key information from the burgeoning volume of scientific papers remains a formidable challenge. this paper introduces an innovative framework designed to automate the extraction of vital data from scientific pdf documents, enabling researchers to discern future research trajectories more readily. autoie uniquely integrates four novel components: (1) a multi-semantic feature fusion-based approach for pdf document layout analysis; (2) advanced functional block recognition in scientific texts; (3) a synergistic technique for extracting and correlating information on molecular sieve synthesis; (4) an online learning paradigm tailored for molecular sieve literature. our sbert model achieves high marco f1 scores of 87.19 and 89.65 on conll04 and ade datasets. in addition, a practical application of autoie in the petrochemical molecular sieve synthesis domain demonstrates its efficacy, evidenced by an impressive 78\% accuracy rate. this research paves the way for enhanced data management and interpretation in molecular sieve synthesis. it is a valuable asset for seasoned experts and newcomers in this specialized field.",2024-01-30
"57","fine-tuned large language models for symptom recognition from spanish clinical text","mai a. shaaban, abbas akkasi, adnan khan, majid komeili, mohammad yaqub","computation and language","the accurate recognition of symptoms in clinical reports is significantly important in the fields of healthcare and biomedical natural language processing. these entities serve as essential building blocks for clinical information extraction, enabling retrieval of critical medical insights from vast amounts of textual data. furthermore, the ability to identify and categorize these entities is fundamental for developing advanced clinical decision support systems, aiding healthcare professionals in diagnosis and treatment planning. in this study, we participated in symptemist, a shared task on the detection of symptoms, signs and findings in spanish medical documents. we combine a set of large language models fine-tuned with the data released by the organizers.",2024-01-28
"58","cross-space adaptive filter: integrating graph topology and node attributes for alleviating the over-smoothing problem","chen huang, haoyang li, yifan zhang, wenqiang lei, jiancheng lv","machine learning","the vanilla graph convolutional network (gcn) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when gcn goes deep. to this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. however, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep gcns, especially when dealing with disassortative graphs. in this paper, we propose a cross-space adaptive filter, called csf, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. then, we cast the topology-based low-pass filter as a mercer's kernel within the context of gcns. this serves as a foundation for combining it with the attribute-based filter to capture the adaptive-frequency information. finally, we derive the cross-space filter via an effective multiple-kernel learning strategy, which unifies the attribute-based high-pass filter and the topology-based low-pass filter. this helps to address the over-smoothing problem while maintaining effectiveness. extensive experiments demonstrate that csf not only successfully alleviates the over-smoothing problem but also promotes the effectiveness of the node classification task.",2024-01-26
"59","looking right is sometimes right: investigating the capabilities of decoder-only llms for sequence labeling","david dukiƒá, jan ≈°najder","computation and language","pre-trained language models based on masked language modeling (mlm) excel in natural language understanding (nlu) tasks. while fine-tuned mlm-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (llms) perform on par with smaller mlm-based encoders. although their performance improves with scale, llms fall short of achieving state-of-the-art results in information extraction (ie) tasks, many of which are formulated as sequence labeling (sl). we hypothesize that llms' poor sl performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. yet, how exactly and to what extent llms' performance on sl can be improved remains unclear. we explore techniques for improving the sl performance of open llms on ie tasks by applying layer-wise removal of the causal mask (cm) during llm fine-tuning. this approach yields performance gains competitive with state-of-the-art sl models, matching or outperforming the results of cm removal from all blocks. our findings hold for diverse sl tasks, demonstrating that open llms with layer-dependent cm removal outperform strong mlm-based encoders and even instruction-tuned llms.",2024-01-25
"60","longhealth: a question answering benchmark with long clinical documents","lisa adams, felix busch, tianyu han, jean-baptiste excoffier, matthieu ortala, alexander l√∂ser, hugo jwl. aerts, jakob nikolas kather, daniel truhn, keno bressem","computation and language","background: recent advancements in large language models (llms) offer potential benefits in healthcare, particularly in processing extensive patient records. however, existing benchmarks do not fully assess llms' capability in handling real-world, lengthy clinical data.
methods: we present the longhealth benchmark, comprising 20 detailed fictional patient cases across various diseases, with each case containing 5,090 to 6,754 words. the benchmark challenges llms with 400 multiple-choice questions in three categories: information extraction, negation, and sorting, challenging llms to extract and interpret information from large clinical documents.
results: we evaluated nine open-source llms with a minimum of 16,000 tokens and also included openai's proprietary and cost-efficient gpt-3.5 turbo for comparison. the highest accuracy was observed for mixtral-8x7b-instruct-v0.1, particularly in tasks focused on information retrieval from single and multiple patient documents. however, all models struggled significantly in tasks requiring the identification of missing information, highlighting a critical area for improvement in clinical data interpretation.
conclusion: while llms show considerable potential for processing long clinical documents, their current accuracy levels are insufficient for reliable clinical use, especially in scenarios requiring the identification of missing information. the longhealth benchmark provides a more realistic assessment of llms in a healthcare setting and highlights the need for further model refinement for safe and effective clinical application.
we make the benchmark and evaluation code publicly available.",2024-01-25
"61","genie: achieving human parity in content-grounded datasets generation","asaf yehudai, boaz carmeli, yosi mass, ofir arviv, nathaniel mills, assaf toledo, eyal shnarch, leshem choshen","computation and language","the lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. to address this gap, we propose genie, a novel method for automatically generating high-quality content-grounded data. it consists of three stages: (a) content preparation, (b) generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) filtering mechanism aiming to ensure the quality and faithfulness of the generated data. we showcase this methodology by generating three large-scale synthetic data, making wishes, for long-form question-answering (lfqa), summarization, and information extraction. in a human evaluation, our generated data was found to be natural and of high quality. furthermore, we compare models trained on our data with models trained on human-written data -- eli5 and asqa for lfqa and cnn-dailymail for summarization. we show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. finally, we applied our method to create lfqa data within the medical domain and compared a model trained on it with models trained on other domains.",2024-01-25
"62","a comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification","madhumita sushil, travis zack, divneet mandair, zhiwei zheng, ahmed wali, yan-ning yu, yuwei quan, atul j. butte","computation and language","although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. meanwhile, large language models (llms) have demonstrated promising transfer learning capability. in this study, we explored whether recent llms can reduce the need for large-scale data annotations. we curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the gpt-4 model and the gpt-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (lstm-att), and the ucsf-bert model. across all 13 tasks, the gpt-4 model performed either significantly better than or as well as the best supervised model, the lstm-att model (average macro f1 score of 0.83 vs. 0.75). on tasks with high imbalance between labels, the differences were more prominent. frequent sources of gpt-4 errors included inferences from multiple samples and complex task design. on complex tasks where large annotated datasets cannot be easily collected, llms can reduce the burden of large-scale data labeling. however, if the use of llms is prohibitive, the use of simpler supervised models with large annotated datasets can provide comparable results. llms demonstrated the potential to speed up the execution of clinical nlp studies by reducing the need for curating large annotated datasets. this may result in an increase in the utilization of nlp-based variables and outcomes in observational clinical studies.",2024-01-25
"63","fine-grained contract ner using instruction based model","hiranmai sri adibhatla, pavan baswani, manish shrivastava","information retrieval","lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. they achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. despite these advancements, the performance of large language models (llms) in information extraction tasks like named entity recognition (ner), using prompts or instructions, still falls short of supervised baselines. the reason for this performance gap can be attributed to the fundamental disparity between ner and llms. ner is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. in contrast, llms are designed as a text generation task. this distinction between semantic labeling and text generation leads to subpar performance. in this paper, we transform the ner task into a text-generation task that can be readily adapted by llms. this involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language. we harness the strength of llms by integrating supervised learning within them. the goal of this combined strategy is to boost the performance of llms in extraction tasks like ner while simultaneously addressing hallucination issues often observed in llm-generated content. a novel corpus contract ner comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models. our models and dataset are available to the community for future research * .",2024-01-24
"64","instructdoc: a dataset for zero-shot generalization of visual document understanding with instructions","ryota tanaka, taichi iki, kyosuke nishida, kuniko saito, jun suzuki","computer vision and pattern recognition","we study the problem of completing various visual document understanding (vdu) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. to this end, we propose instructdoc, the first large-scale collection of 30 publicly available vdu datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. furthermore, to enhance the generalization performance on vdu tasks, we design a new instruction-based document reading and understanding model, instructdr, that connects document images, image encoders, and large language models (llms) through a trainable bridging module. experiments demonstrate that instructdr can effectively adapt to new vdu datasets, tasks, and domains via given instructions and outperforms existing multimodal llms and chatgpt without specific training.",2024-01-24
"65","nlp-based relation extraction methods in requirements engineering","quim motger, xavier franch","software engineering","in the context of requirements engineering, relation extraction is the task of documenting the traceability between requirements artefacts. when dealing with textual requirements (i.e., requirements expressed using natural language), relation extraction becomes a cognitively challenging task, especially in terms of ambiguity and required effort from domain-experts. hence, in highly-adaptive, large-scale environments, effective and efficient automated relation extraction using natural language processing techniques becomes essential.  in this chapter, we present a comprehensive overview of natural language-based relation extraction from text-based requirements. we initially describe the fundamentals of requirements relations based on the most relevant literature in the field, including the most common requirements relations types. the core of the chapter is composed by two main sections: (i) natural language techniques for the identification and categorization of requirements relations (i.e., syntactic vs. semantic techniques), and (ii) information extraction methods for the task of relation extraction (i.e., retrieval-based vs. machine learning-based methods). we complement this analysis with the state-of-the-art challenges and the envisioned future research directions. overall, this chapter aims at providing a clear perspective on the theoretical and practical fundamentals in the field of natural language-based relation extraction.",2024-01-22
"66","mind: improving multimodal sentiment analysis via multimodal information disentanglement","weichen dai, xingyu li, pengbo hu, zeyu wang, ji qi, jianlin peng, yi zhou","multimedia","learning effective joint representations has been a central task in multimodal sentiment analysis. previous methods focus on leveraging the correlations between different modalities and enhancing performance through sophisticated fusion techniques. however, challenges still exist due to the inherent heterogeneity of distinct modalities, which may lead to distributional gap, impeding the full exploitation of inter-modal information and resulting in redundancy and impurity in the information extracted from features. to address this problem, we introduce the multimodal information disentanglement (mind) approach. mind decomposes the multimodal inputs into a modality-invariant component, a modality-specific component, and a remnant noise component for each modality through a shared encoder and multiple private encoders. the shared encoder aims to explore the shared information and commonality across modalities, while the private encoders are deployed to capture the distinctive information and characteristic features. these representations thus furnish a comprehensive perspective of the multimodal data, facilitating the fusion process instrumental for subsequent prediction tasks. furthermore, mind improves the learned representations by explicitly modeling the task-irrelevant noise in an adversarial manner. experimental evaluations conducted on benchmark datasets, including cmu-mosi, cmu-mosei, and ur-funny, demonstrate mind's superior performance over existing state-of-the-art methods in both multimodal emotion recognition and multimodal humor detection tasks.",2024-01-22
"67","exploiting duality in open information extraction with predicate prompt","zhen chen, jingping liu, deqing yang, yanghua xiao, huimin xu, zongyu wang, rui xie, yunsen xian","computation and language","open information extraction (openie) aims to extract the schema-free triplets in the form of (\emph{subject}, \emph{predicate}, \emph{object}) from a given sentence. compared with general information extraction (ie), openie poses more challenges for the ie models, {especially when multiple complicated triplets exist in a sentence. to extract these complicated triplets more effectively, in this paper we propose a novel generative openie model, namely \emph{dualoie}, which achieves a dual task at the same time as extracting some triplets from the sentence, i.e., converting the triplets into the sentence.} such dual task encourages the model to correctly recognize the structure of the given sentence and thus is helpful to extract all potential triplets from the sentence. specifically, dualoie extracts the triplets in two steps: 1) first extracting a sequence of all potential predicates, 2) then using the predicate sequence as a prompt to induce the generation of triplets. our experiments on two benchmarks and our dataset constructed from meituan demonstrate that dualoie achieves the best performance among the state-of-the-art baselines. furthermore, the online a/b test on meituan platform shows that 0.93\% improvement of qv-ctr and 0.56\% improvement of uv-ctr have been obtained when the triplets extracted by dualoie were leveraged in meituan's search system.",2024-01-20
"68","mining experimental data from materials science literature with large language models","luca foppiano, guillaume lambard, toshiyuki amagasa, masashi ishii","computation and language","this study is dedicated to evaluating the capabilities of advanced large language models (llms) such as gpt-3.5-turbo, gpt-4, and gpt-4-turbo in the extraction of structured information from scientific documents within the field of materials science. we introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. to this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (ner) of studied materials and physical properties and (ii) a relation extraction (re) between these entities. the performance of llms in executing these tasks is benchmarked against traditional models based on the bert architecture and rule-based approaches. for ner, llms fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. however, for re, a gpt-3.5-turbo fine-tuned with the appropriate strategy outperforms all models, including the baseline. without any fine-tuning, gpt-4 and gpt-4-turbo display remarkable reasoning and relationship extraction capabilities after being provided with merely a couple of examples, surpassing the baseline. overall, the results suggest that although llms demonstrate relevant reasoning skills in connecting concepts, for tasks requiring extracting complex domain-specific entities like materials, specialised models are currently a better choice.",2024-01-19
"69","name tagging under domain shift via metric learning for life sciences","hongyi liu, qingyun wang, payam karisani, heng ji","computation and language","name tagging is a key component of information extraction (ie), particularly in scientific domains such as biomedicine and chemistry, where large language models (llms), e.g., chatgpt, fall short. we investigate the applicability of transfer learning for enhancing a name tagging model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). a common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. in our experiments we observed that such a model is prone to mis-labeling the source entities, which can often appear in the text, as the target entities. to alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, however, at the same time, to project the source entities and target entities into separate regions of the feature space. this diminishes the risk of mis-labeling the source entities as the target entities. our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. we carry out our extensive experiments across three source and three target datasets, and demonstrate that our method outperforms the baselines, in some scenarios by 5\% absolute value.",2024-01-19
"70","large language models for scientific information extraction: an empirical study for virology","mahsa shamsabadi, jennifer d'souza, s√∂ren auer","computation and language","in this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like wikipedia infoboxes or structured amazon product descriptions. these representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. our novel automated approach leverages the robust text generation capabilities of llms to produce structured scholarly contribution summaries, offering both a practical solution and insights into llms' emergent abilities.
for llms, the prime focus is on improving their general intelligence as conversational agents. we argue that these models can also be applied effectively in information extraction (ie), specifically in complex ie tasks within terse domains like science. this paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. our results show that finetuned flan-t5 with 1000x fewer parameters than the state-of-the-art gpt-davinci is competitive for the task.",2024-01-18
"71","distantly supervised morpho-syntactic model for relation extraction","nicolas gutehrl√©, iana atanassova","computation and language","the task of information extraction (ie) involves automatically converting unstructured textual content into structured data. most research in this field concentrates on extracting all facts or a specific set of relationships from documents. in this paper, we present a method for the extraction and categorisation of an unrestricted set of relationships from text. our method relies on morpho-syntactic extraction patterns obtained by a distant supervision method, and creates syntactic and semantic indices to extract and classify candidate graphs. we evaluate our approach on six datasets built on wikidata and wikipedia. the evaluation shows that our approach can achieve precision scores of up to 0.85, but with lower recall and f1 scores. our approach allows to quickly create rule-based systems for information extraction and to build annotated datasets to train machine-learning and deep-learning based classifiers.",2024-01-18
"72","sketch-guided constrained decoding for boosting blackbox large language models without logit access","saibo geng, berkay d√∂ner, chris wendler, martin josifoski, robert west","computation and language","constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (llms). this paper introduces sketch-guided constrained decoding (sgcd), a novel approach to constrained decoding for blackbox llms, which operates without access to the logits of the blackbox llm. sgcd utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox llm, effectively treating this initial output as a ""sketch"" for further elaboration. this approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. we demonstrate the efficacy of sgcd through experiments in closed information extraction and constituency parsing, showing how it enhances the utility and flexibility of blackbox llms for complex nlp tasks.",2024-01-18
"73","decades of transformation: evolution of the nasa astrophysics data system's infrastructure","alberto accomazzi","instrumentation and methods for astrophysics","the nasa astrophysics data system (ads) is the primary digital library portal for researchers in astronomy and astrophysics. over the past 30 years, the ads has gone from being an astronomy-focused bibliographic database to an open digital library system supporting research in space and (soon) earth sciences. this paper describes the evolution of the ads system, its capabilities, and the technological infrastructure underpinning it.
we give an overview of the ads's original architecture, constructed primarily around simple database models. this bespoke system allowed for the efficient indexing of metadata and citations, the digitization and archival of full-text articles, and the rapid development of discipline-specific capabilities running on commodity hardware. the move towards a cloud-based microservices architecture and an open-source search engine in the late 2010s marked a significant shift, bringing full-text search capabilities, a modern api, higher uptime, more reliable data retrieval, and integration of advanced visualizations and analytics.
another crucial evolution came with the gradual and ongoing incorporation of machine learning and natural language processing algorithms in our data pipelines. originally used for information extraction and classification tasks, nlp and ml techniques are now being developed to improve metadata enrichment, search, notifications, and recommendations. we describe how these computational techniques are being embedded into our software infrastructure, the challenges faced, and the benefits reaped.
finally, we conclude by describing the future prospects of ads and its ongoing expansion, discussing the challenges of managing an interdisciplinary information system in the era of ai and open science, where information is abundant, technology is transformative, but their trustworthiness can be elusive.",2024-01-18
"74","texturedreamer: image-guided texture synthesis through geometry-aware diffusion","yu-ying yeh, jia-bin huang, changil kim, lei xiao, thu nguyen-phuoc, numair khan, cheng zhang, manmohan chandraker, carl s marshall, zhao dong, zhengqin li","computer vision and pattern recognition","we present texturedreamer, a novel image-guided texture synthesis method to transfer relightable textures from a small number of input images (3 to 5) to target 3d shapes across arbitrary categories. texture creation is a pivotal challenge in vision and graphics. industrial companies hire experienced artists to manually craft textures for 3d assets. classical methods require densely sampled views and accurately aligned geometry, while learning-based methods are confined to category-specific shapes within the dataset. in contrast, texturedreamer can transfer highly detailed, intricate textures from real-world environments to arbitrary objects with only a few casually captured images, potentially significantly democratizing texture creation. our core idea, personalized geometry-aware score distillation (pgsd), draws inspiration from recent advancements in diffuse models, including personalized modeling for texture information extraction, variational score distillation for detailed appearance synthesis, and explicit geometry guidance with controlnet. our integration and several essential modifications substantially improve the texture quality. experiments on real images spanning different categories show that texturedreamer can successfully transfer highly realistic, semantic meaningful texture to arbitrary objects, surpassing the visual quality of previous state-of-the-art.",2024-01-17
"75","univie: a unified label space approach to visual information extraction from form-like documents","kai hu, jiawei wang, weihong lin, zhuoyao zhong, lei sun, qiang huo","computation and language","existing methods for visual information extraction (vie) from form-like documents typically fragment the process into separate subtasks, such as key information extraction, key-value pair extraction, and choice group extraction. however, these approaches often overlook the hierarchical structure of form documents, including hierarchical key-value pairs and hierarchical choice groups. to address these limitations, we present a new perspective, reframing vie as a relation prediction problem and unifying labels of different tasks into a single label space. this unified approach allows for the definition of various relation types and effectively tackles hierarchical relationships in form-like documents. in line with this perspective, we present univie, a unified model that addresses the vie problem comprehensively. univie functions using a coarse-to-fine strategy. it initially generates tree proposals through a tree proposal network, which are subsequently refined into hierarchical trees by a relation decoder module. to enhance the relation prediction capabilities of univie, we incorporate two novel tree constraints into the relation decoder: a tree attention mask and a tree level embedding. extensive experimental evaluations on both our in-house dataset hierforms and a publicly available dataset sibr, substantiate that our method achieves state-of-the-art results, underscoring the effectiveness and potential of our unified approach in advancing the field of vie.",2024-01-17
"76","s3m: semantic segmentation sparse mapping for uavs with rgb-d camera","thanh nguyen canh, van-truong nguyen, xiem hoangvan, armagan elibol, nak young chong","robotics","unmanned aerial vehicles (uavs) hold immense potential for critical applications, such as search and rescue operations, where accurate perception of indoor environments is paramount. however, the concurrent amalgamation of localization, 3d reconstruction, and semantic segmentation presents a notable hurdle, especially in the context of uavs equipped with constrained power and computational resources. this paper presents a novel approach to address challenges in semantic information extraction and utilization within uav operations. our system integrates state-of-the-art visual slam to estimate a comprehensive 6-dof pose and advanced object segmentation methods at the back end. to improve the computational and storage efficiency of the framework, we adopt a streamlined voxel-based 3d map representation - octomap to build a working system. furthermore, the fusion algorithm is incorporated to obtain the semantic information of each frame from the front-end slam task, and the corresponding point. by leveraging semantic information, our framework enhances the uav's ability to perceive and navigate through indoor spaces, addressing challenges in pose estimation accuracy and uncertainty reduction. through gazebo simulations, we validate the efficacy of our proposed system and successfully embed our approach into a jetson xavier agx unit for real-world applications.",2024-01-16
"77","object-oriented semantic mapping for reliable uavs navigation","thanh nguyen canh, armagan elibol, nak young chong, xiem hoangvan","robotics","to autonomously navigate in real-world environments, special in search and rescue operations, unmanned aerial vehicles (uavs) necessitate comprehensive maps to ensure safety. however, the prevalent metric map often lacks semantic information crucial for holistic scene comprehension. in this paper, we proposed a system to construct a probabilistic metric map enriched with object information extracted from the environment from rgb-d images. our approach combines a state-of-the-art yolov8-based object detection framework at the front end and a 2d slam method - cartographer at the back end. to effectively track and position semantic object classes extracted from the front-end interface, we employ the innovative bot-sort methodology. a novel association method is introduced to extract the position of objects and then project it with the metric map. unlike previous research, our approach takes into reliable navigating in the environment with various hollow bottom objects. the output of our system is a probabilistic map, which significantly enhances the map's representation by incorporating object-specific attributes, encompassing class distinctions, accurate positioning, and object heights. a number of experiments have been conducted to evaluate our proposed approach. the results show that the robot can effectively produce augmented semantic maps containing several objects (notably chairs and desks). furthermore, our system is evaluated within an embedded computer - jetson xavier agx unit to demonstrate the use case in real-world applications.",2024-01-16
"78","embre: entity-aware masking for biomedical relation extraction","mingjie li, karin verspoor","computation and language","information extraction techniques, including named entity recognition (ner) and relation extraction (re), are crucial in many domains to support making sense of vast amounts of unstructured text data by identifying and connecting relevant information. such techniques can assist researchers in extracting valuable insights. in this paper, we introduce the entity-aware masking for biomedical relation extraction (embre) method for biomedical relation extraction, as applied in the context of the biored challenge task 1, in which human-annotated entities are provided as input. specifically, we integrate entity knowledge into a deep neural network by pretraining the backbone model with an entity masking objective. we randomly mask named entities for each instance and let the model identify the masked entity along with its type. in this way, the model is capable of learning more specific knowledge and more robust representations. then, we utilize the pre-trained model as our backbone to encode language representations and feed these representations into two multilayer perceptron (mlps) to predict the logits for relation and novelty, respectively. the experimental results demonstrate that our proposed method can improve the performances of entity pair, relation and novelty extraction over our baseline.",2024-01-15
"79","assessing the effectiveness of binary-level cfi techniques","ruturaj k. vaidya, prasad a. kulkarni","cryptography and security","memory corruption is an important class of vulnerability that can be leveraged to craft control flow hijacking attacks. control flow integrity (cfi) provides protection against such attacks. application of type-based cfi policies requires information regarding the number and type of function arguments. binary-level type recovery is inherently speculative, which motivates the need for an evaluation framework to assess the effectiveness of binary-level cfi techniques compared with their source-level counterparts, where such type information is fully and accurately accessible. in this work, we develop a novel, generalized and extensible framework to assess how the program analysis information we get from state-of-the-art binary analysis tools affects the efficacy of type-based cfi techniques. we introduce new and insightful metrics to quantitatively compare source independent cfi policies with their ground truth source aware counterparts. we leverage our framework to evaluate binary-level cfi policies implemented using program analysis information extracted from the ida pro binary analyzer and compared with the ground truth information obtained from the llvm compiler, and present our observations.",2024-01-13
"80","yoio: you only iterate once by mining and fusing multiple necessary global information in the optical flow estimation","yu jing, tan yujuan, ren ao, liu duo","computer vision and pattern recognition","occlusions pose a significant challenge to optical flow algorithms that even rely on global evidences. we consider an occluded point to be one that is imaged in the reference frame but not in the next. estimating the motion of these points is extremely difficult, particularly in the two-frame setting. previous work only used the current frame as the only input, which could not guarantee providing correct global reference information for occluded points, and had problems such as long calculation time and poor accuracy in predicting optical flow at occluded points. to enable both high accuracy and efficiency, we fully mine and utilize the spatiotemporal information provided by the frame pair, design a loopback judgment algorithm to ensure that correct global reference information is obtained, mine multiple necessary global information, and design an efficient refinement module that fuses these global information. specifically, we propose a yoio framework, which consists of three main components: an initial flow estimator, a multiple global information extraction module, and a unified refinement module. we demonstrate that optical flow estimates in the occluded regions can be significantly improved in only one iteration without damaging the performance in non-occluded regions. compared with gma, the optical flow prediction accuracy of this method in the occluded area is improved by more than 10%, and the occ_out area exceeds 15%, while the calculation time is 27% shorter. this approach, running up to 18.9fps with 436*1024 image resolution, obtains new state-of-the-art results on the challenging sintel dataset among all published and unpublished approaches that can run in real-time, suggesting a new paradigm for accurate and efficient optical flow estimation.",2024-01-11
"81","inacia: integrating large language models in brazilian audit courts: opportunities and challenges","jayr pereira, andre assumpcao, julio trecenti, luiz airosa, caio lente, jhonatan cl√©to, guilherme dobins, rodrigo nogueira, luis mitchell, roberto lotufo","computation and language","this paper introduces inacia (instru√ß√£o assistida com intelig√™ncia artificial), a groundbreaking system designed to integrate large language models (llms) into the operational framework of brazilian federal court of accounts (tcu). the system automates various stages of case analysis, including basic information extraction, admissibility examination, periculum in mora and fumus boni iuris analyses, and recommendations generation. through a series of experiments, we demonstrate inacia's potential in extracting relevant information from case documents, evaluating its legal plausibility, and formulating propositions for judicial decision-making. utilizing a validation dataset alongside llms, our evaluation methodology presents a novel approach to assessing system performance, correlating highly with human judgment. these results underscore inacia's potential in complex legal task handling while also acknowledging the current limitations. this study discusses possible improvements and the broader implications of applying ai in legal contexts, suggesting that inacia represents a significant step towards integrating ai in legal systems globally, albeit with cautious optimism grounded in the empirical findings.",2024-01-10
"82","from low resource information extraction to identifying influential nodes in knowledge graphs","erica cai, olga simek, benjamin a. miller, danielle sullivan-pao, evan young, christopher l. smith","social and information networks","we propose a pipeline for identifying important entities from intelligence reports that constructs a knowledge graph, where nodes correspond to entities of fine-grained types (e.g. traffickers) extracted from the text and edges correspond to extracted relations between entities (e.g. cartel membership). the important entities in intelligence reports then map to central nodes in the knowledge graph. we introduce a novel method that extracts fine-grained entities in a few-shot setting (few labeled examples), given limited resources available to label the frequently changing entity types that intelligence analysts are interested in. it outperforms other state-of-the-art methods. next, we identify challenges facing previous evaluations of zero-shot (no labeled examples) methods for extracting relations, affecting the step of populating edges. finally, we explore the utility of the pipeline: given the goal of identifying important entities, we evaluate the impact of relation extraction errors on the identification of central nodes in several real and synthetic networks. the impact of these errors varies significantly by graph topology, suggesting that confidence in measurements based on automatically extracted relations should depend on observed network features.",2024-01-10
"83","non-deterministic extension of plasma wind tunnel data calibrated model predictions to flight conditions","przemyslaw rostkowski, jeremie b.e. meurisse, marco panesi","applications","this work proposes a novel approach for non-deterministic extension of experimental data that considers structural model inadequacy for conditions other than the calibration scenario while simultaneously resolving any significant prior-data discrepancy with information extracted from flight measurements. this functionality is achieved through methodical utilization of model error emulators and bayesian model averaging studies with available response data. the outlined approach does not require prior flight data availability and introduces straightforward mechanisms for their assimilation in future predictions. application of the methodology is demonstrated herein by extending material performance data captured at the hymets facility to the msl scenario, where the described process yields results that exhibit significantly improved capacity for predictive uncertainty quantification studies. this work also investigates limitations associated with straightforward uncertainty propagation procedures onto calibrated model predictions for the flight scenario and manages computational requirements with sensitivity analysis and surrogate modeling techniques.",2024-01-09
"84","captain at coliee 2023: efficient methods for legal information retrieval and entailment tasks","chau nguyen, phuong nguyen, thanh tran, dat nguyen, an trieu, tin pham, anh dang, le-minh nguyen","computation and language","the competition on legal information extraction/entailment (coliee) is held annually to encourage advancements in the automatic processing of legal texts. processing legal documents is challenging due to the intricate structure and meaning of legal language. in this paper, we outline our strategies for tackling task 2, task 3, and task 4 in the coliee 2023 competition. our approach involved utilizing appropriate state-of-the-art deep learning methods, designing methods based on domain characteristics observation, and applying meticulous engineering practices and methodologies to the competition. as a result, our performance in these tasks has been outstanding, with first places in task 2 and task 3, and promising results in task 4. our source code is available at this https url.",2024-01-07
"85","umie: unified multimodal information extraction with instruction tuning","lin sun, kai zhang, qingyuan li, renze lou","artificial intelligence","multimodal information extraction (mie) gains significant attention as the popularity of multimedia content increases. however, current mie methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across mie tasks. to address these issues, we propose umie, a unified multimodal information extractor to unify three mie tasks as a generation problem using instruction tuning, being able to effectively extract both textual and visual mentions. extensive experiments show that our single umie outperforms various state-of-the-art (sota) methods across six mie datasets on three tasks. furthermore, in-depth analysis demonstrates umie's strong generalization in the zero-shot setting, robustness to instruction variants, and interpretability. our research serves as an initial step towards a unified mie model and initiates the exploration into both instruction tuning and large language models within the mie domain. our code, data, and model are available at this https url",2024-01-05
"86","fine-tuning and utilization methods of domain-specific llms","cheonsu jeong","computation and language","recent releases of pre-trained large language models (llms) have gained considerable traction, yet research on fine-tuning and employing domain-specific llms remains scarce. this study investigates approaches for fine-tuning and leveraging domain-specific llms, highlighting trends in llms, foundational models, and methods for domain-specific pre-training. focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for llm fine-tuning in finance. addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. in the practical application of llm fine-tuning, the study outlines the procedure and implementation for generating domain-specific llms in finance. various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extraction, and customer service enhancement, are exemplified. the study explores the potential of llms in the financial domain, identifies limitations, and proposes directions for improvement, contributing valuable insights for future research. ultimately, it advances natural language processing technology in business, suggesting proactive llm utilization in financial services across industries.",2024-01-01
"87","docgraphlm: documental graph language model for information extraction","dongsheng wang, zhiqiang ma, armineh nourbakhsh, kang gu, sameena shah","computation and language","advances in visually rich document understanding (vrdu) have enabled information extraction and question answering over documents with complex layouts. two tropes of architectures have emerged -- transformer-based models inspired by llms, and graph neural networks. in this paper, we introduce docgraphlm, a novel framework that combines pre-trained language models with graph semantics. to achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. docgraphlm predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. our experiments on three sota datasets show consistent improvement on ie and qa tasks with the adoption of graph features. moreover, we report that adopting the graph features accelerates convergence in the learning process during training, despite being solely constructed through link prediction.",2024-01-05
"88","concurrent brainstorming & hypothesis satisfying: an iterative framework for enhanced retrieval-augmented generation (r2cbr3h-sr)","arash shahmansoori","information theory","addressing the complexity of comprehensive information retrieval, this study introduces an innovative, iterative retrieval-augmented generation system. our approach uniquely integrates a vector-space driven re-ranking mechanism with concurrent brainstorming to expedite the retrieval of highly relevant documents, thereby streamlining the generation of potential queries. this sets the stage for our novel hybrid process, which synergistically combines hypothesis formulation with satisfying decision-making strategy to determine content adequacy, leveraging a chain of thought-based prompting technique. this unified hypothesize-satisfied phase intelligently distills information to ascertain whether user queries have been satisfactorily addressed. upon reaching this criterion, the system refines its output into a concise representation, maximizing conceptual density with minimal verbosity. the iterative nature of the workflow enhances process efficiency and accuracy. crucially, the concurrency within the brainstorming phase significantly accelerates recursive operations, facilitating rapid convergence to solution satisfaction. compared to conventional methods, our system demonstrates a marked improvement in computational time and cost-effectiveness. this research advances the state-of-the-art in intelligent retrieval systems, setting a new benchmark for resource-efficient information extraction and abstraction in knowledge-intensive applications.",2024-01-03
"89","enhancing representation in medical vision-language foundation models via multi-scale information extraction techniques","weijian huang, cheng li, hong-yu zhou, jiarun liu, hao yang, yong liang, guangming shi, hairong zheng, shanshan wang","computer vision and pattern recognition","the development of medical vision-language foundation models has attracted significant attention in the field of medicine and healthcare due to their promising prospect in various clinical applications. while previous studies have commonly focused on feature learning at a single learning scale, investigation on integrating multi-scale information is lacking, which may hinder the potential for mutual reinforcement among these features. this paper aims to bridge this gap by proposing a method that effectively exploits multi-scale information to enhance the performance of medical foundation models. the proposed method simultaneously exploits features at the local, instance, modality and global aspects, facilitating comprehensive representation learning within the models. we evaluate the effectiveness of the proposed method on six open-source datasets across different clinical tasks, demonstrating its ability to enhance the performance of medical foundation models.",2024-01-03
"90","an autoregressive text-to-graph framework for joint entity and relation extraction","urchade zaratiana, nadi tomeh, pierre holat, thierry charnois","computation and language","in this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. in contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. it generates a linearized graph where nodes represent text spans and edges represent relation triplets. our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. code is available at this https url.",2024-01-02
"91","l3cube-mahasocialner: a social media based marathi ner dataset and bert models","harsh chaudhari, anuja patil, dhanashree lavekar, pranav khairnar, raviraj joshi","computation and language","this work introduces the l3cube-mahasocialner dataset, the first and largest social media dataset specifically designed for named entity recognition (ner) in the marathi language. the dataset comprises 18,000 manually labeled sentences covering eight entity classes, addressing challenges posed by social media data, including non-standard language and informal idioms. deep learning models, including cnn, lstm, bilstm, and transformer models, are evaluated on the individual dataset with iob and non-iob notations. the results demonstrate the effectiveness of these models in accurately recognizing named entities in marathi informal text. the l3cube-mahasocialner dataset offers user-centric information extraction and supports real-time applications, providing a valuable resource for public opinion analysis, news, and marketing on social media platforms. we also show that the zero-shot results of the regular ner model are poor on the social ner test set thus highlighting the need for more social ner datasets. the datasets and models are publicly available at this https url",2023-12-30
"92","large language models for generative information extraction: a survey","derong xu, wei chen, wenjun peng, chao zhang, tong xu, xiangyu zhao, xian wu, yefeng zheng, enhong chen","computation and language","information extraction (ie) aims to extract structural knowledge (such as entities, relations, and events) from plain natural language texts. recently, generative large language models (llms) have demonstrated remarkable capabilities in text understanding and generation, allowing for generalization across various domains and tasks. as a result, numerous works have been proposed to harness abilities of llms and offer viable solutions for ie tasks based on a generative paradigm. to conduct a comprehensive systematic review and exploration of llm efforts for ie tasks, in this study, we survey the most recent advancements in this field. we first present an extensive overview by categorizing these works in terms of various ie subtasks and learning paradigms, then we empirically analyze the most advanced methods and discover the emerging trend of ie tasks with llms. based on thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. we maintain a public repository and consistently update related resources at: \url{this https url}.",2023-12-29
"93","commonsense for zero-shot natural language video localization","meghana holla, ismini lourentzou","computer vision and pattern recognition","zero-shot natural language-video localization (nlvl) methods have exhibited promising results in training nlvl models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. however, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. in this paper, we investigate the effectiveness of commonsense reasoning in zero-shot nlvl. specifically, we present coronet, a zero-shot nlvl framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. coronet employs graph convolution networks (gcn) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. through empirical evaluations on two benchmark datasets, we demonstrate that coronet surpasses both zero-shot and weakly supervised baselines, achieving improvements up to 32.13% across various recall thresholds and up to 6.33% in miou. these results underscore the significance of leveraging commonsense reasoning for zero-shot nlvl.",2023-12-29
"94","noncommutative information is revealed from the static detector outside the black hole","jingran shi, yipeng liu, baocheng zhang","high energy physics - theory","we investigate the transition behavior of the two-level atom as the unruh-dewitt detector outside a noncommutative black hole. when the mass of the black hole is small enough, the difference between the commutative and noncommutative black hole can be distinguished. in particular, an evident fluctuation appearing at a far distance from the horizon by calculating the quantum fisher information of the transition rate with regard to the local hawking temperature provides a novel and interesting result about the information extraction of the noncommutativity for a small-mass black hole.",2023-12-28
"95","autonomous docking method via non-linear model predictive control","roni permana saputra, midriem mirdanies, eko joni pristianto, dayat kurniawan","robotics","this paper presents a proposed method of autonomous control for docking tasks of a single-seat personal mobility vehicle. we proposed a non-linear model predictive control (nmpc) based visual servoing to achieves the desired autonomous docking task. the proposed method is implemented on a four-wheel electric wheelchair platform, with two independent rear driving wheels and two front castor wheels. the nmpc-based visual servoing technique leverages the information extracted from a visual sensor as a real-time feedback for the nmpc to control the motion of the vehicle achieving the desired autonomous docking task. to evaluate the performance of the proposed controller method, a number of experiments both in simulation and in the actual setting. the controller performance is then evaluated based on the controller design requirement. the simulation results on autonomous docking experiments show that the proposed controller has been successfully achieve the desired controller design requirement to generate realtime trajectory for the vehicle performing autonomous docking tasks in several different scenarios.",2023-12-27
"96","solving label variation in scientific information extraction via multi-task learning","dong pham, xanh ho, quang-thuy ha, akiko aizawa","computation and language","scientific information extraction (scientificie) is a critical task that involves the identification of scientific entities and their relationships. the complexity of this task is compounded by the necessity for domain-specific knowledge and the limited availability of annotated data. two of the most popular datasets for scientificie are semeval-2018 task-7 and scierc. they have overlapping samples and differ in their annotation schemes, which leads to conflicts. in this study, we first introduced a novel approach based on multi-task learning to address label variations. we then proposed a soft labeling technique that converts inconsistent labels into probabilistic distributions. the experimental results demonstrated that the proposed method can enhance the model robustness to label noise and improve the end-to-end performance in both scientificie tasks. the analysis revealed that label variations can be particularly effective in handling ambiguous instances. furthermore, the richness of the information captured by label variations can potentially reduce data size requirements. the findings highlight the importance of releasing variation labels and promote future research on other tasks in other domains. overall, this study demonstrates the effectiveness of multi-task learning and the potential of label variations to enhance the performance of scientificie.",2023-12-25
"97","yayi-uie: a chat-enhanced instruction tuning framework for universal information extraction","xinglin xiao, yijie wang, nan xu, yuqi wang, hanxuan yang, minzheng wang, yin luo, lei wang, wenji mao, daniel zeng","computation and language","the difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. recent work has proposed methods based on large language models to uniformly model different information extraction tasks. however, these existing methods are deficient in their information extraction capabilities for chinese languages other than english. in this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (yayi-uie), which supports both chinese and english. specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. experimental results show that our proposed framework achieves state-of-the-art performance on chinese datasets while also achieving comparable performance on english datasets under both supervised settings and zero-shot settings.",2023-12-24
"98","pixel-level change detection pseudo-label learning for remote sensing change captioning","chenyang liu, keyan chen, zipeng qi, haotian zhang, zhengxia zou, zhenwei shi","computer vision and pattern recognition","the existing methods for remote sensing image change captioning (rsicc) perform well in simple scenes but exhibit poorer performance in complex scenes. this limitation is primarily attributed to the model's constrained visual ability to distinguish and locate changes. acknowledging the inherent correlation between change detection (cd) and rsicc tasks, we believe pixel-level cd is significant for describing the differences between images through language. regrettably, the current rsicc dataset lacks readily available pixel-level cd labels. to address this deficiency, we leverage a model trained on existing cd datasets to derive cd pseudo-labels. we propose an innovative network with an auxiliary cd branch, supervised by pseudo-labels. furthermore, a semantic fusion augment (sfa) module is proposed to fuse the feature information extracted by the cd branch, thereby facilitating the nuanced description of changes. experiments demonstrate that our method achieves state-of-the-art performance and validate that learning pixel-level cd pseudo-labels significantly contributes to change captioning. our code will be available at: this https url",2023-12-23
"99","a joint communication and computation design for semantic wireless communication with probability graph","zhouxiang zhao, zhaohui yang, xu gan, quoc-viet pham, chongwen huang, wei xu, zhaoyang zhang","information theory","in this paper, we delve into the challenge of optimizing joint communication and computation for semantic communication over wireless networks using a probability graph framework. in the considered model, the base station (bs) extracts the small-sized compressed semantic information through removing redundant messages based on the stored knowledge base. specifically, the knowledge base is encapsulated in a probability graph that encapsulates statistical relations. at the user side, the compressed information is accurately deduced using the same probability graph employed by the bs. while this approach introduces an additional computational overhead for semantic information extraction, it significantly curtails communication resource consumption by transmitting concise data. we derive both communication and computation cost models based on the inference process of the probability graph. building upon these models, we introduce a joint communication and computation resource allocation problem aimed at minimizing the overall energy consumption of the network, while accounting for latency, power, and semantic constraints. to address this problem, we obtain a closed-form solution for transmission power under a fixed semantic compression ratio. subsequently, we propose an efficient linear search-based algorithm to attain the optimal solution for the considered problem with low computational complexity. simulation results underscore the effectiveness of our proposed system, showcasing notable improvements compared to conventional non-semantic schemes.",2023-12-21
"100","data transformation to construct a dataset for generating entity-relationship model from natural language","zhenwen li, jian-guang lou, tao xie","computation and language","in order to reduce the manual cost of designing er models, recent approaches have been proposed to address the task of nl2erm, i.e., automatically generating entity-relationship (er) models from natural language (nl) utterances such as software requirements. these approaches are typically rule-based ones, which rely on rigid heuristic rules; these approaches cannot generalize well to various linguistic ways of describing the same requirement. despite having better generalization capability than rule-based approaches, deep-learning-based models are lacking for nl2erm due to lacking a large-scale dataset. to address this issue, in this paper, we report our insight that there exists a high similarity between the task of nl2erm and the increasingly popular task of text-to-sql, and propose a data transformation algorithm that transforms the existing data of text-to-sql into the data of nl2erm. we apply our data transformation algorithm on spider, one of the most popular text-to-sql datasets, and we also collect some data entries with different nl types, to obtain a large-scale nl2erm dataset. because nl2erm can be seen as a special information extraction (ie) task, we train two state-of-the-art ie models on our dataset. the experimental results show that both the two models achieve high performance and outperform existing baselines.",2023-12-21
"101","dyblurf: dynamic deblurring neural radiance fields for blurry monocular video","minh-quan viet bui, jongmin park, jihyong oh, munchurl kim","computer vision and pattern recognition","video view synthesis, allowing for the creation of visually appealing frames from arbitrary viewpoints and times, offers immersive viewing experiences. neural radiance fields, particularly nerf, initially developed for static scenes, have spurred the creation of various methods for video view synthesis. however, the challenge for video view synthesis arises from motion blur, a consequence of object or camera movement during exposure, which hinders the precise synthesis of sharp spatio-temporal views. in response, we propose a novel dynamic deblurring nerf framework for blurry monocular video, called dyblurf, consisting of an interleave ray refinement (irr) stage and a motion decomposition-based deblurring (mdd) stage. our dyblurf is the first that addresses and handles the novel view synthesis for blurry monocular video. the irr stage jointly reconstructs dynamic 3d scenes and refines the inaccurate camera pose information to combat imprecise pose information extracted from the given blurry frames. the mdd stage is a novel incremental latent sharp-rays prediction (ilsp) approach for the blurry monocular video frames by decomposing the latent sharp rays into global camera motion and local object motion components. extensive experimental results demonstrate that our dyblurf outperforms qualitatively and quantitatively the very recent state-of-the-art methods. our project page including source codes and pretrained model are publicly available at this https url.",2023-12-21
"102","task-oriented semantics-aware communications for robotic waypoint transmission: the value and age of information approach","wenchao wu, yuanqing yang, yansha deng, a. hamid aghvami","robotics","the ultra-reliable and low-latency communication (urllc) service of the fifth-generation (5g) mobile communication network struggles to support safe robot operation. nowadays, the sixth-generation (6g) mobile communication network is proposed to provide hyper-reliable and low-latency communication to enable safer control for robots. however, current 5g/ 6g research mainly focused on improving communication performance, while the robotics community mostly assumed communication to be ideal. to jointly consider communication and robotic control with a focus on the specific robotic task, we propose task-oriented and semantics-aware communication in robotic control (tsrc) to exploit the context of data and its importance in achieving the task at both transmitter and receiver. at the transmitter, we propose a deep reinforcement learning algorithm to generate optimal control and command (c&c) data and a proactive repetition scheme (deeppro) to increase the successful transmission probability. at the receiver, we design the value of information (voi) and age of information (aoi) based queue ordering mechanism (va-qom) to reorganize the queue based on the semantic information extracted from the aoi and the voi. the simulation results validate that our proposed tsrc framework achieves a 91.5% improvement in the mean square error compared to the traditional unmanned aerial vehicle control framework.",2023-12-20
"103","progressive frequency-aware network for laparoscopic image desmoking","jiale zhang, wenfeng huang, xiangyun liao, qiong wang","image and video processing","laparoscopic surgery offers minimally invasive procedures with better patient outcomes, but smoke presence challenges visibility and safety. existing learning-based methods demand large datasets and high computational resources. we propose the progressive frequency-aware network (pfan), a lightweight gan framework for laparoscopic image desmoking, combining the strengths of cnn and transformer for progressive information extraction in the frequency domain. pfan features cnn-based multi-scale bottleneck-inverting (mbi) blocks for capturing local high-frequency information and locally-enhanced axial attention transformers (lat) for efficiently handling global low-frequency information. pfan efficiently desmokes laparoscopic images even with limited training data. our method outperforms state-of-the-art approaches in psnr, ssim, ciede2000, and visual quality on the cholec80 dataset and retains only 629k parameters. our code and models are made publicly available at: this https url.",2023-12-19
"104","agent-based learning of materials datasets from scientific literature","mehrad ansari, seyed mohamad moosavi","artificial intelligence","advancements in machine learning and artificial intelligence are transforming materials discovery. yet, the availability of structured experimental data remains a bottleneck. the vast corpus of scientific literature presents a valuable and rich resource of such data. however, manual dataset creation from these resources is challenging due to issues in maintaining quality and consistency, scalability limitations, and the risk of human error and bias. therefore, in this work, we develop a chemist ai agent, powered by large language models (llms), to overcome these challenges by autonomously creating structured datasets from natural language text, ranging from sentences and paragraphs to extensive scientific research articles. our chemist ai agent, eunomia, can plan and execute actions by leveraging the existing knowledge from decades of scientific research articles, scientists, the internet and other tools altogether. we benchmark the performance of our approach in three different information extraction tasks with various levels of complexity, including solid-state impurity doping, metal-organic framework (mof) chemical formula, and property relations. our results demonstrate that our zero-shot agent, with the appropriate tools, is capable of attaining performance that is either superior or comparable to the state-of-the-art fine-tuned materials information extraction methods. this approach simplifies compilation of machine learning-ready datasets for various materials discovery applications, and significantly ease the accessibility of advanced natural language processing tools for novice users in natural language. the methodology in this work is developed as an open-source software on this https url.",2023-12-18
"105","perceptual musical features for interpretable audio tagging","vassilis lyberatos, spyridon kantarelis, edmund dervakos, giorgos stamou","sound","in the age of music streaming platforms, the task of automatically tagging music audio has garnered significant attention, driving researchers to devise methods aimed at enhancing performance metrics on standard datasets. most recent approaches rely on deep neural networks, which, despite their impressive performance, possess opacity, making it challenging to elucidate their output for a given input. while the issue of interpretability has been emphasized in other fields like medicine, it has not received attention in music-related tasks. in this study, we explored the relevance of interpretability in the context of automatic music tagging. we constructed a workflow that incorporates three different information extraction techniques: a) leveraging symbolic knowledge, b) utilizing auxiliary deep neural networks, and c) employing signal processing to extract perceptual features from audio files. these features were subsequently used to train an interpretable machine-learning model for tag prediction. we conducted experiments on two datasets, namely the mtg-jamendo dataset and the gtzan dataset. our method surpassed the performance of baseline models in both tasks and, in certain instances, demonstrated competitiveness with the current state-of-the-art. we conclude that there are use cases where the deterioration in performance is outweighed by the value of interpretability.",2023-12-18
"106","language-conditioned learning for robotic manipulation: a survey","hongkuan zhou, xiangtong yao, yuan meng, siming sun, zhenshan bing, kai huang, alois knoll","robotics","language-conditioned robotic manipulation represents a cutting-edge area of research, enabling seamless communication and cooperation between humans and robotic agents. this field focuses on teaching robotic systems to comprehend and execute instructions conveyed in natural language. to achieve this, the development of robust language understanding models capable of extracting actionable insights from textual input is essential. in this comprehensive survey, we systematically explore recent advancements in language-conditioned approaches within the context of robotic manipulation. we analyze these approaches based on their learning paradigms, which encompass reinforcement learning, imitation learning, and the integration of foundational models, such as large language models and vision-language models. furthermore, we conduct an in-depth comparative analysis, considering aspects like semantic information extraction, environment & evaluation, auxiliary tasks, and task representation. finally, we outline potential future research directions in the realm of language-conditioned learning for robotic manipulation, with the topic of generalization capabilities and safety issues. the github repository of this paper can be found at this https url",2023-12-17
"107","hyperpie: hyperparameter information extraction from scientific publications","tarek saier, mayumi ohta, takuto asakura, michael f√§rber","computation and language","automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. the extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. an important type of information not covered by existing approaches is hyperparameters. in this paper, we formalize and tackle hyperparameter information extraction (hyperpie) as an entity recognition and relation extraction task. we create a labeled data set covering publications from a variety of computer science disciplines. using this data set, we train and evaluate bert-based fine-tuned models as well as five large language models: gpt-3.5, galactica, falcon, vicuna, and wizardlm. for fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% f1 over a state-of-the-art baseline. for large language models, we develop an approach leveraging yaml output for structured data extraction, which achieves an average improvement of 5.5% f1 in entity recognition over using json. with our best performing model we extract hyperparameter information from a large number of unannotated papers, and analyze patterns across disciplines. all our data and source code is publicly available at this https url",2023-12-17
"108","gear-up: generative ai and external knowledge-based retrieval upgrading scholarly article searches for systematic reviews","kaushik roy, vedant khandelwal, harshul surana, valerie vera, amit sheth, heather heckman","information retrieval","systematic reviews (srs) - the librarian-assisted literature survey of scholarly articles takes time and requires significant human resources. given the ever-increasing volume of published studies, applying existing computing and informatics technology can decrease this time and resource burden. due to the revolutionary advances in (1) generative ai such as chatgpt, and (2) external knowledge-augmented information extraction efforts such as retrieval-augmented generation, in this work, we explore the use of techniques from (1) and (2) for sr. we demonstrate a system that takes user queries, performs query expansion to obtain enriched context (includes additional terms and definitions by querying language models and knowledge graphs), and uses this context to search for articles on scholarly databases to retrieve articles. we perform qualitative evaluations of our system through comparison against sentinel (ground truth) articles provided by an in-house librarian. the demo can be found at: this https url.",2023-12-15
"109","information extraction from unstructured data using augmented-ai and computer vision","aditya parikh","computer vision and pattern recognition","process of information extraction (ie) is often used to extract meaningful information from unstructured and unlabeled data. conventional methods of data extraction including application of ocr and passing extraction engine, are inefficient on large data and have their limitation. in this paper, a peculiar technique of information extraction is proposed using a2i and computer vision technologies, which also includes nlp.",2023-12-15
"110","training-free zero-shot composed image retrieval with local concept reranking","shitong sun, fanghua ye, shaogang gong","computer vision and pattern recognition","composed image retrieval attempts to retrieve an image of interest from gallery images through a composed query of a reference image and its corresponding modified text. it has recently attracted attention due to the collaboration of information-rich images and concise language to precisely express the requirements of target images. most of the existing composed image retrieval methods follow a supervised learning paradigm to perform training on a costly triplet dataset composed of a reference image, modified text, and a corresponding target image. to alleviate the demand for difficult-to-obtain labeled triplet data, recent methods have introduced zero-shot composed image retrieval (zs-cir), which aims to retrieve the target image without the supervision of human-labeled triplets but instead relies on image-text pairs or self-generated triplets. however, these methods are less computationally efficient due to the requirement of training and also less understandable, assuming that the interaction between image and text is conducted with implicit query embedding. in this work, we present a new training-free zero-shot composed image retrieval (tfcir) method which translates the query into explicit human-understandable text. this helps improve computation efficiency while maintaining the generalization of foundation models. further, we introduce a local concept reranking (lcr) mechanism to focus on discriminative local information extracted from the modified instruction. extensive experiments on three zs-cir benchmarks show that the proposed approach can achieve comparable performances with state-of-the-art methods and significantly outperforms other training-free methods on the open domain datasets, cirr and circo, as well as the fashion domain dataset, fashioniq.",2023-12-14
"111","encoder-minimal and decoder-minimal framework for remote sensing image dehazing","yuanbo wen, tao gao, ziqi li, jing zhang, ting chen","computer vision and pattern recognition","haze obscures remote sensing images, hindering valuable information extraction. to this end, we propose rshazenet, an encoder-minimal and decoder-minimal framework for efficient remote sensing image dehazing. specifically, regarding the process of merging features within the same level, we develop an innovative module called intra-level transposed fusion module (itfm). this module employs adaptive transposed self-attention to capture comprehensive context-aware information, facilitating the robust context-aware feature fusion. meanwhile, we present a cross-level multi-view interaction module (cmim) to enable effective interactions between features from various levels, mitigating the loss of information due to the repeated sampling operations. in addition, we propose a multi-view progressive extraction block (mpeb) that partitions the features into four distinct components and employs convolution with varying kernel sizes, groups, and dilation factors to facilitate view-progressive feature learning. extensive experiments demonstrate the superiority of our proposed rshazenet. we release the source code and all pre-trained models at \url{this https url}.",2023-12-13
"112","ai-driven structure detection and information extraction from historical cadastral maps (early 19th century franciscean cadastre in the province of styria) and current high-resolution satellite and aerial imagery for remote sensing","wolfgang g√∂derle, christian macher, katrin mauthner, oliver pimas, fabian rampetsreiter","computer vision and pattern recognition","cadastres from the 19th century are a complex as well as rich source for historians and archaeologists, whose use presents them with great challenges. for archaeological and historical remote sensing, we have trained several deep learning models, cnns as well as vision transformers, to extract large-scale data from this knowledge representation. we present the principle results of our work here and we present a the demonstrator of our browser-based tool that allows researchers and public stakeholders to quickly identify spots that featured buildings in the 19th century franciscean cadastre. the tool not only supports scholars and fellow researchers in building a better understanding of the settlement history of the region of styria, it also helps public administration and fellow citizens to swiftly identify areas of heightened sensibility with regard to the cultural heritage of the region.",2023-12-08
"113","classification of retail products: from probabilistic ranking to neural networks","manar mohamed hafez, rebeca p. d√≠az redondo, ana fern√°ndez-vilas, h√©ctor olivera paz√≥","artificial intelligence","food retailing is now on an accelerated path to a success penetration into the digital market by new ways of value creation at all stages of the consumer decision process. one of the most important imperatives in this path is the availability of quality data to feed all the process in digital transformation. but the quality of data is not so obvious if we consider the variety of products and suppliers in the grocery market. within this context of digital transformation of grocery industry, \textit{midiadia} is spanish data provider company that works on converting data from the retailers' products into knowledge with attributes and insights from the product labels, that is, maintaining quality data in a dynamic market with a high dispersion of products. currently, they manually categorize products (groceries) according to the information extracted directly (text processing) from the product labelling and packaging. this paper introduces a solution to automatically categorize the constantly changing product catalogue into a 3-level food taxonomy. our proposal studies three different approaches: a score-based ranking method, traditional machine learning algorithms, and deep neural networks. thus, we provide four different classifiers that support a more efficient and less error-prone maintenance of groceries catalogues, the main asset of the company. finally, we have compared the performance of these three alternatives, concluding that traditional machine learning algorithms perform better, but closely followed by the score-based approach.",2023-12-12
"114","transformer-based no-reference image quality assessment via supervised contrastive learning","jinsong shi, pan gao, jie qin","computer vision and pattern recognition","image quality assessment (iqa) has long been a research hotspot in the field of image processing, especially no-reference image quality assessment (nr-iqa). due to the powerful feature extraction ability, existing convolution neural network (cnn) and transformers based nr-iqa methods have achieved considerable progress. however, they still exhibit limited capability when facing unknown authentic distortion datasets. to further improve nr-iqa performance, in this paper, a novel supervised contrastive learning (scl) and transformer-based nr-iqa model satqa is proposed. we first train a model on a large-scale synthetic dataset by scl (no image subjective score is required) to extract degradation features of images with various distortion types and levels. to further extract distortion information from images, we propose a backbone network incorporating the multi-stream block (msb) by combining the cnn inductive bias and transformer long-term dependence modeling capability. finally, we propose the patch attention block (pab) to obtain the final distorted image quality score by fusing the degradation features learned from contrastive learning with the perceptual distortion information extracted by the backbone network. experimental results on seven standard iqa datasets show that satqa outperforms the state-of-the-art methods for both synthetic and authentic datasets. code is available at this https url",2023-12-12
"115","unsupervised extractive summarization with learnable length control strategies","renlong jie, xiaojun meng, xin jiang, qun liu","artificial intelligence","unsupervised extractive summarization is an important technique in information extraction and retrieval. compared with supervised method, it does not require high-quality human-labelled summaries for training and thus can be easily applied for documents with different types, domains or languages. most of existing unsupervised methods including textrank and pacsum rely on graph-based ranking on sentence centrality. however, this scorer can not be directly applied in end-to-end training, and the positional-related prior assumption is often needed for achieving good summaries. in addition, less attention is paid to length-controllable extractor, where users can decide to summarize texts under particular length constraint. this paper introduces an unsupervised extractive summarization model based on a siamese network, for which we develop a trainable bidirectional prediction objective between the selected summary and the original document. different from the centrality-based ranking methods, our extractive scorer can be trained in an end-to-end manner, with no other requirement of positional assumption. in addition, we introduce a differentiable length control module by approximating 0-1 knapsack solver for end-to-end length-controllable extracting. experiments show that our unsupervised method largely outperforms the centrality-based baseline using a same sentence encoder. in terms of length control ability, via our trainable knapsack module, the performance consistently outperforms the strong baseline without utilizing end-to-end training. human evaluation further evidences that our method performs the best among baselines in terms of relevance and consistency.",2023-12-12
"116","asf-yolo: a novel yolo model with attentional scale sequence fusion for cell instance segmentation","ming kang, chee-ming ting, fung fung ting, rapha√´l c.-w. phan","computer vision and pattern recognition","we propose a novel attentional scale sequence fusion based you only look once (yolo) framework (asf-yolo) which combines spatial and scale features for accurate and fast cell instance segmentation. built on the yolo segmentation framework, we employ the scale sequence feature fusion (ssff) module to enhance the multi-scale information extraction capability of the network, and the triple feature encoder (tpe) module to fuse feature maps of different scales to increase detailed information. we further introduce a channel and position attention mechanism (cpam) to integrate both the ssff and tpe modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance. experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed asf-yolo model. it achieves a box map of 0.91, mask map of 0.887, and an inference speed of 47.3 fps on the 2018 data science bowl dataset, outperforming the state-of-the-art methods. the source code is available at this https url.",2023-12-11
"117","visiontraj: a noise-robust trajectory recovery framework based on large-scale camera network","zhishuai li, ziyue li, xiaoru hu, guoqing du, yunhao nie, feng zhu, lei bai, rui zhao","computer vision and pattern recognition","trajectory recovery based on the snapshots from the city-wide multi-camera network facilitates urban mobility sensing and driveway optimization. the state-of-the-art solutions devoted to such a vision-based scheme typically incorporate predefined rules or unsupervised iterative feedback, struggling with multi-fold challenges such as lack of open-source datasets for training the whole pipeline, and the vulnerability to the noises from visual inputs. in response to the dilemma, this paper proposes visiontraj, the first learning-based model that reconstructs vehicle trajectories from snapshots recorded by road network cameras. coupled with it, we elaborate on two rational vision-trajectory datasets, which produce extensive trajectory data along with corresponding visual snapshots, enabling supervised vision-trajectory interplay extraction. following the data creation, based on the results from the off-the-shelf multi-modal vehicle clustering, we first re-formulate the trajectory recovery problem as a generative task and introduce the canonical transformer as the autoregressive backbone. then, to identify clustering noises (e.g., false positives) with the bound on the snapshots' spatiotemporal dependencies, a gcn-based soft-denoising module is conducted based on the fine- and coarse-grained re-id clusters. additionally, we harness strong semantic information extracted from the tracklet to provide detailed insights into the vehicle's entry and exit actions during trajectory recovery. the denoising and tracklet components can also act as plug-and-play modules to boost baselines. experimental results on the two hand-crafted datasets show that the proposed visiontraj achieves a maximum +11.5% improvement against the sub-best model.",2023-12-11
"118","using program knowledge graph to uncover software vulnerabilities","m. xie, t. rahat, w. wang, y. tian","cryptography and security","in an increasingly interconnected and data-driven world, the importance of robust security measures cannot be overstated. a knowledge graph constructed with information extracted from the system along with the desired security behavior can be utilized to identify complex security vulnerabilities hidden underneath the systems. unfortunately, existing security knowledge graphs are constructed from coarse-grained information extracted from publicly available vulnerability reports, which are not equipped to check actual security violations in real-world system implementations. in this poster, we present a novel approach of using program knowledge graph that is embedded with fine-grained execution information of the systems (e.g., callgraph, data-flow, etc.) along with information extracted from the public vulnerability and weakness datasets (e.g., cve and cwe). we further demonstrate that our custom security knowledge graph can be checked against the standard queries generated by llm, providing a powerful way to identify security vulnerabilities and weaknesses in critical systems.",2023-12-08
"119","scalable knowledge graph construction and inference on human genome variants","shivika prasanna, deepthi rao, eduardo simoes, praveen rao","artificial intelligence","real-world knowledge can be represented as a graph consisting of entities and relationships between the entities. the need for efficient and scalable solutions arises when dealing with vast genomic data, like rna-sequencing. knowledge graphs offer a powerful approach for various tasks in such large-scale genomic data, such as analysis and inference. in this work, variant-level information extracted from the rna-sequences of vaccine-na√Øve covid-19 patients have been represented as a unified, large knowledge graph. variant call format (vcf) files containing the variant-level information were annotated to include further information for each variant. the data records in the annotated files were then converted to resource description framework (rdf) triples. each vcf file obtained had an associated cadd scores file that contained the raw and phred-scaled scores for each variant. an ontology was defined for the vcf and cadd scores files. using this ontology and the extracted information, a large, scalable knowledge graph was created. available graph storage was then leveraged to query and create datasets for further downstream tasks. we also present a case study using the knowledge graph and perform a classification task using graph machine learning. we also draw comparisons between different graph neural networks (gnns) for the case study.",2023-12-07
"120","seeing the random forest through the decision trees. supporting learning health systems from histopathology with machine learning models: challenges and opportunities","ricardo gonzalez, ashirbani saha, clinton j.v. campbell, peyman nejat, cynthia lokker, andrew p. norgan","computer vision and pattern recognition","this paper discusses some overlooked challenges faced when working with machine learning models for histopathology and presents a novel opportunity to support ""learning health systems"" with them. initially, the authors elaborate on these challenges after separating them according to their mitigation strategies: those that need innovative approaches, time, or future technological capabilities and those that require a conceptual reappraisal from a critical perspective. then, a novel opportunity to support ""learning health systems"" by integrating hidden information extracted by ml models from digitalized histopathology slides with other healthcare big data is presented.",2023-12-06
"121","leveraging ai-derived data for carbon accounting: information extraction from alternative sources","olamide oladeji, seyed shahabeddin mousavi","computation and language","carbon accounting is a fundamental building block in our global path to emissions reduction and decarbonization, yet many challenges exist in achieving reliable and trusted carbon accounting measures. we motivate that carbon accounting not only needs to be more data-driven, but also more methodologically sound. we discuss the need for alternative, more diverse data sources that can play a significant role on our path to trusted carbon accounting procedures and elaborate on not only why, but how artificial intelligence (ai) in general and natural language processing (nlp) in particular can unlock reasonable access to a treasure trove of alternative data sets in light of the recent advances in the field that better enable the utilization of unstructured data in this process. we present a case study of the recent developments on real-world data via an nlp-powered analysis using openai's gpt api on financial and shipping data. we conclude the paper with a discussion on how these methods and approaches can be integrated into a broader framework for ai-enabled integrative carbon accounting.",2023-11-26
"122","lazy-k: decoding for constrained token classification","arthur hemmer, micka√´l coustaty, nicola bartolo, j√©r√¥me brachat, jean-marc ogier","computation and language","we explore the possibility of improving probabilistic models in structured prediction. specifically, we combine the models with constrained decoding approaches in the context of token classification for information extraction. the decoding methods search for constraint-satisfying label-assignments while maximizing the total probability. to do this, we evaluate several existing approaches, as well as propose a novel decoding method called lazy-$k$. our findings demonstrate that constrained decoding approaches can significantly improve the models' performances, especially when using smaller models. the lazy-$k$ approach allows for more flexibility between decoding time and accuracy. the code for using lazy-$k$ decoding can be found here: this https url.",2023-12-06
"123","rethinking e-commerce search","haixun wang, taesik na","information retrieval","e-commerce search and recommendation usually operate on structured data such as product catalogs and taxonomies. however, creating better search and recommendation systems often requires a large variety of unstructured data including customer reviews and articles on the web. traditionally, the solution has always been converting unstructured data into structured data through information extraction, and conducting search over the structured data. however, this is a costly approach that often has low quality. in this paper, we envision a solution that does entirely the opposite. instead of converting unstructured data (web pages, customer reviews, etc) to structured data, we instead convert structured data (product inventory, catalogs, taxonomies, etc) into textual data, which can be easily integrated into the text corpus that trains llms. then, search and recommendation can be performed through a q/a mechanism through an llm instead of using traditional information retrieval methods over structured data.",2023-12-06
"124","resin-editor: a schema-guided hierarchical event graph visualizer and editor","khanh duy nguyen, zixuan zhang, reece suchocki, sha li, martha palmer, susan brown, jiawei han, heng ji","human-computer interaction","in this paper, we present resin-editor, an interactive event graph visualizer and editor designed for analyzing complex events. our resin-editor system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from human-curated event schemas. resin-editor's unique features include hierarchical graph visualization, comprehensive source tracing, and interactive user editing, which is more powerful and versatile than existing information extraction (ie) visualization tools. in our evaluation of resin-editor, we demonstrate ways in which our tool is effective in understanding complex events and enhancing system performance. the source code, a video demonstration, and a live website for resin-editor have been made publicly available.",2023-12-05
"125","generator born from classifier","runpeng yu, xinchao wang","machine learning","in this paper, we make a bold attempt toward an ambitious task: given a pre-trained classifier, we aim to reconstruct an image generator, without relying on any data samples. from a black-box perspective, this challenge seems intractable, since it inevitably involves identifying the inverse function for a classifier, which is, by nature, an information extraction process. as such, we resort to leveraging the knowledge encapsulated within the parameters of the neural network. grounded on the theory of maximum-margin bias of gradient descent, we propose a novel learning paradigm, in which the generator is trained to ensure that the convergence conditions of the network parameters are satisfied over the generated distribution of the samples. empirical validation from various image generation tasks substantiates the efficacy of our strategy.",2023-12-05
"126","llms accelerate annotation for medical information extraction","akshay goel, almog gueta, omry gilon, chang liu, sofia erell, lan huong nguyen, xiaohong hao, bolous jaber, shashir reddy, rupesh kartha, jean steiner, itay laish, amir feder","computation and language","the unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. to uncover this hidden information, specialized natural language processing (nlp) models are required. however, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. in this paper, we propose an approach that combines large language models (llms) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. by utilizing llms in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. we rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. the results highlight the potential of using llms to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored nlp solutions in healthcare.",2023-12-04
"127","mobileutr: revisiting the relationship between light-weight cnn and transformer for efficient medical image segmentation","fenghe tang, bingkun nian, jianrui ding, quan quan, jie yang, wei liu, s.kevin zhou","image and video processing","due to the scarcity and specific imaging characteristics in medical images, light-weighting vision transformers (vits) for efficient medical image segmentation is a significant challenge, and current studies have not yet paid attention to this issue. this work revisits the relationship between cnns and transformers in lightweight universal networks for medical image segmentation, aiming to integrate the advantages of both worlds at the infrastructure design level. in order to leverage the inductive bias inherent in cnns, we abstract a transformer-like lightweight cnns block (convutr) as the patch embeddings of vits, feeding transformer with denoised, non-redundant and highly condensed semantic information. moreover, an adaptive local-global-local (lgl) block is introduced to facilitate efficient local-to-global information flow exchange, maximizing transformer's global context information extraction capabilities. finally, we build an efficient medical image segmentation model (mobileutr) based on cnn and transformer. extensive experiments on five public medical image datasets with three different modalities demonstrate the superiority of mobileutr over the state-of-the-art methods, while boasting lighter weights and lower computational cost. code is available at this https url.",2023-12-04
"128","explanatory argument extraction of correct answers in resident medical exams","iakes goenaga, aitziber atutxa, koldo gojenola, maite oronoz, rodrigo agerri","computation and language","developing the required technology to assist medical experts in their everyday activities is currently a hot topic in the artificial intelligence research field. thus, a number of large language models (llms) and automated benchmarks have recently been proposed with the aim of facilitating information extraction in evidence-based medicine (ebm) using natural language as a tool for mediating in human-ai interaction. the most representative benchmarks are limited to either multiple-choice or long-form answers and are available only in english. in order to address these shortcomings, in this paper we present a new dataset which, unlike previous work: (i) includes not only explanatory arguments for the correct answer, but also arguments to reason why the incorrect answers are not correct; (ii) the explanations are written originally by medical doctors to answer questions from the spanish residency medical exams. furthermore, this new benchmark allows us to setup a novel extractive task which consists of identifying the explanation of the correct answer written by medical doctors. an additional benefit of our setting is that we can leverage the extractive qa paradigm to automatically evaluate performance of llms without resorting to costly manual evaluation by medical experts. comprehensive experimentation with language models for spanish shows that sometimes multilingual models fare better than monolingual ones, even outperforming models which have been adapted to the medical domain. furthermore, results across the monolingual models are mixed, with supposedly smaller and inferior models performing competitively. in any case, the obtained results show that our novel dataset and approach can be an effective technique to help medical practitioners in identifying relevant evidence-based explanations for medical questions.",2023-12-01
"129","a metadata generation system with semantic understanding for video retrieval in film production","feilin han, zhaoxu meng","multimedia","in film production, metadata plays an important role in original raw video indexing and classification within the industrial post-production software. inspired by deep visual-semantic methods, we propose an automated image information extraction process to extend the diversity of metadata entities for massive large-scale raw video searching and retrieval. in this paper, we introduce the proposed system architecture and modules, integrating semantic annotation models and user-demand-oriented information fusion. we conducted experiments to validate the effectiveness of our system on film raw video semantic annotation dataset (film-rvsad) and slate board template dataset (sbtd), two benchmark datasets built for cinematography-related semantic annotation and slate detection. experimental results show that the proposed system provides an effective strategy to improve the efficiency of metadata generation and transformation, which is necessary and convenient for collaborative work in the filmmaking process.",2023-11-30
"130","sparsedc: depth completion from sparse and non-uniform inputs","chen long, wenxiao zhang, zhe chen, haiping wang, yuan liu, zhen cao, zhen dong, bisheng yang","computer vision and pattern recognition","we propose sparsedc, a model for depth completion of sparse and non-uniform depth inputs. unlike previous methods focusing on completing fixed distributions on benchmark datasets (e.g., nyu with 500 points, kitti with 64 lines), sparsedc is specifically designed to handle depth maps with poor quality in real usage. the key contributions of sparsedc are two-fold. first, we design a simple strategy, called sffm, to improve the robustness under sparse input by explicitly filling the unstable depth features with stable image features. second, we propose a two-branch feature embedder to predict both the precise local geometry of regions with available depth values and accurate structures in regions with no depth. the key of the embedder is an uncertainty-based fusion module called uffm to balance the local and long-term information extracted by cnns and vits. extensive indoor and outdoor experiments demonstrate the robustness of our framework when facing sparse and non-uniform input depths. the pre-trained model and code are available at this https url.",2023-11-30
"131","accurate segmentation of optic disc and cup from multiple pseudo-labels by noise-aware learning","tengjin weng, yang shen, zhidong zhao, zhiming cheng, shuai wang","computer vision and pattern recognition","optic disc and cup segmentation play a crucial role in automating the screening and diagnosis of optic glaucoma. while data-driven convolutional neural networks (cnns) show promise in this area, the inherent ambiguity of segmenting object and background boundaries in the task of optic disc and cup segmentation leads to noisy annotations that impact model performance. to address this, we propose an innovative label-denoising method of multiple pseudo-labels noise-aware network (mpnn) for accurate optic disc and cup segmentation. specifically, the multiple pseudo-labels generation and guided denoising (mpggd) module generates pseudo-labels by multiple different initialization networks trained on true labels, and the pixel-level consensus information extracted from these pseudo-labels guides to differentiate clean pixels from noisy pixels. the training framework of the mpnn is constructed by a teacher-student architecture to learn segmentation from clean pixels and noisy pixels. particularly, such a framework adeptly leverages (i) reliable and fundamental insights from clean pixels and (ii) the supplementary knowledge within noisy pixels via multiple perturbation-based unsupervised consistency. compared to other label-denoising methods, comprehensive experimental results on the riga dataset demonstrate our method's excellent performance and significant denoising ability.",2023-11-30
"132","esg accountability made easy: docqa at your service","lokesh mishra, cesar berrospi, kasper dinkla, diego antognini, francesco fusco, benedikt bothur, maksym lysak, nikolaos livathinos, ahmed nassar, panagiotis vagenas, lucas morin, christoph auer, michele dolfi, peter staar","computation and language","we present deep search docqa. this application enables information extraction from documents via a question-answering conversational assistant. the system integrates several technologies from different ai disciplines consisting of document conversion to machine-readable format (via computer vision), finding relevant data (via natural language processing), and formulating an eloquent response (via large language models). users can explore over 10,000 environmental, social, and governance (esg) disclosure reports from over 2000 corporations. the deep search platform can be accessed at: this https url.",2023-11-30
"133","aviationgpt: a large language model for the aviation domain","liya wang, jason chou, xin zhou, alex tien, diane m baumgartner","computation and language","the advent of chatgpt and gpt-4 has captivated the world with large language models (llms), demonstrating exceptional performance in question-answering, summarization, and content generation. the aviation industry is characterized by an abundance of complex, unstructured text data, replete with technical jargon and specialized terminology. moreover, labeled data for model building are scarce in this domain, resulting in low usage of aviation text data. the emergence of llms presents an opportunity to transform this situation, but there is a lack of llms specifically designed for the aviation domain. to address this gap, we propose aviationgpt, which is built on open-source llama-2 and mistral architectures and continuously trained on a wealth of carefully curated aviation datasets. experimental results reveal that aviationgpt offers users multiple advantages, including the versatility to tackle diverse natural language processing (nlp) problems (e.g., question-answering, summarization, document writing, information extraction, report querying, data cleaning, and interactive data exploration). it also provides accurate and contextually relevant responses within the aviation domain and significantly improves performance (e.g., over a 40% performance gain in tested cases). with aviationgpt, the aviation industry is better equipped to address more complex research problems and enhance the efficiency and safety of national airspace system (nas) operations.",2023-11-29
"134","improving embedding of graphs with missing data by soft manifolds","andrea marinoni, pietro lio', alessandro barp, christian jutten, mark girolami","machine learning","embedding graphs in continous spaces is a key factor in designing and developing algorithms for automatic information extraction to be applied in diverse tasks (e.g., learning, inferring, predicting). the reliability of graph embeddings directly depends on how much the geometry of the continuous space matches the graph structure. manifolds are mathematical structure that can enable to incorporate in their topological spaces the graph characteristics, and in particular nodes distances. state-of-the-art of manifold-based graph embedding algorithms take advantage of the assumption that the projection on a tangential space of each point in the manifold (corresponding to a node in the graph) would locally resemble a euclidean space. although this condition helps in achieving efficient analytical solutions to the embedding problem, it does not represent an adequate set-up to work with modern real life graphs, that are characterized by weighted connections across nodes often computed over sparse datasets with missing records. in this work, we introduce a new class of manifold, named soft manifold, that can solve this situation. in particular, soft manifolds are mathematical structures with spherical symmetry where the tangent spaces to each point are hypocycloids whose shape is defined according to the velocity of information propagation across the data points. using soft manifolds for graph embedding, we can provide continuous spaces to pursue any task in data analysis over complex datasets. experimental results on reconstruction tasks on synthetic and real datasets show how the proposed approach enable more accurate and reliable characterization of graphs in continuous spaces with respect to the state-of-the-art.",2023-11-29
"135","a quantitative approach to understand self-supervised models as cross-lingual feature extractors","shuyue stella li, beining xu, xiangyu zhang, hexin liu, wenhan chao, leibny paola garcia","computation and language","in this work, we study the features extracted by english self-supervised learning (ssl) models in cross-lingual contexts and propose a new metric to predict the quality of feature representations. using automatic speech recognition (asr) as a downstream task, we analyze the effect of model size, training objectives, and model architecture on the models' performance as a feature extractor for a set of topologically diverse corpora. we develop a novel metric, the phonetic-syntax ratio (psr), to measure the phonetic and synthetic information in the extracted representations using deep generalized canonical correlation analysis. results show the contrastive loss in the wav2vec2.0 objective facilitates more effective cross-lingual feature extraction. there is a positive correlation between psr scores and asr performance, suggesting that phonetic information extracted by monolingual ssl models can be used for downstream tasks in cross-lingual settings. the proposed metric is an effective indicator of the quality of the representations and can be useful for model selection.",2023-11-27
"136","gpt struct me: probing gpt models on narrative entity extraction","hugo sousa, nuno guimar√£es, al√≠pio jorge, ricardo campos","computation and language","the importance of systems that can extract structured information from textual data becomes increasingly pronounced given the ever-increasing volume of text produced on a daily basis. having a system that can effectively extract such information in an interoperable manner would be an asset for several domains, be it finance, health, or legal. recent developments in natural language processing led to the production of powerful language models that can, to some degree, mimic human intelligence. such effectiveness raises a pertinent question: can these models be leveraged for the extraction of structured information? in this work, we address this question by evaluating the capabilities of two state-of-the-art language models -- gpt-3 and gpt-3.5, commonly known as chatgpt -- in the extraction of narrative entities, namely events, participants, and temporal expressions. this study is conducted on the text2story lusa dataset, a collection of 119 portuguese news articles whose annotation framework includes a set of entity structures along with several tags and attribute values. we first select the best prompt template through an ablation study over prompt components that provide varying degrees of information on a subset of documents of the dataset. subsequently, we use the best templates to evaluate the effectiveness of the models on the remaining documents. the results obtained indicate that gpt models are competitive with out-of-the-box baseline systems, presenting an all-in-one alternative for practitioners with limited resources. by studying the strengths and limitations of these models in the context of information extraction, we offer insights that can guide future improvements and avenues to explore in this field.",2023-11-24
"137","eigen: expert-informed joint learning aggregation for high-fidelity information extraction from document images","abhishek singh, venkatapathy subramanian, ayush maheshwari, pradeep narayan, devi prasad shetty, ganesh ramakrishnan","computer vision and pattern recognition","information extraction (ie) from document images is challenging due to the high variability of layout formats. deep models such as layoutlm and bros have been proposed to address this problem and have shown promising results. however, they still require a large amount of field-level annotations for training these models. other approaches using rule-based methods have also been proposed based on the understanding of the layout and semantics of a form such as geometric position, or type of the fields, etc. in this work, we propose a novel approach, eigen (expert-informed joint learning aggreation), which combines rule-based methods with deep learning models using data programming approaches to circumvent the requirement of annotation of large amounts of training data. specifically, eigen consolidates weak labels induced from multiple heuristics through generative models and use them along with a small number of annotated labels to jointly train a deep model. in our framework, we propose the use of labeling functions that include incorporating contextual information thus capturing the visual and language context of a word for accurate categorization. we empirically show that our eigen framework can significantly improve the performance of state-of-the-art deep models with the availability of very few labeled data instances. the source code is available at this https url.",2023-11-23
"138","comparison of pipeline, sequence-to-sequence, and gpt models for end-to-end relation extraction: experiments with the rare disease use-case","shashank gupta, xuguang ai, ramakanth kavuluru","computation and language","end-to-end relation extraction (e2ere) is an important and realistic application of natural language processing (nlp) in biomedicine. in this paper, we aim to compare three prevailing paradigms for e2ere using a complex dataset focused on rare diseases involving discontinuous and nested entities. we use the raredis information extraction dataset to evaluate three competing approaches (for e2ere): ner $\rightarrow$ re pipelines, joint sequence to sequence models, and generative pre-trained transformer (gpt) models. we use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; gpt models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 f1 points. partial matches and discontinuous entities caused many ner errors contributing to lower overall e2e performances. we also verify these findings on a second e2ere dataset for chemical-protein interactions. although generative lm-based methods are more suitable for zero-shot settings, when training data is available, our results show that it is better to work with more conventional models trained and tailored for e2ere. more innovative methods are needed to marry the best of the both worlds from smaller encoder-decoder pipeline models and the larger gpt models to improve e2ere. as of now, we see that well designed pipeline models offer substantial performance gains at a lower cost and carbon footprint for e2ere. our contribution is also the first to conduct e2ere for the raredis dataset.",2023-11-22
"139","cmfdformer: transformer-based copy-move forgery detection with continual learning","yaqi liu, chao xia, song xiao, qingxiao guan, wenqian dong, yifan zhang, nenghai yu","computer vision and pattern recognition","copy-move forgery detection aims at detecting duplicated regions in a suspected forged image, and deep learning based copy-move forgery detection methods are in the ascendant. these deep learning based methods heavily rely on synthetic training data, and the performance will degrade when facing new tasks. in this paper, we propose a transformer-style copy-move forgery detection network named as cmfdformer, and provide a novel pcsd (pooled cube and strip distillation) continual learning framework to help cmfdformer handle new tasks. cmfdformer consists of a mit (mix transformer) backbone network and a phd (pluggable hybrid decoder) mask prediction network. the mit backbone network is a transformer-style network which is adopted on the basis of comprehensive analyses with cnn-style and mlp-style backbones. the phd network is constructed based on self-correlation computation, hierarchical feature integration, a multi-scale cycle fully-connected block and a mask reconstruction block. the phd network is applicable to feature extractors of different styles for hierarchical multi-scale information extraction, achieving comparable performance. last but not least, we propose a pcsd continual learning framework to improve the forgery detectability and avoid catastrophic forgetting when handling new tasks. our continual learning framework restricts intermediate features from the phd network, and takes advantage of both cube pooling and strip pooling. extensive experiments on publicly available datasets demonstrate the good performance of cmfdformer and the effectiveness of the pcsd continual learning framework.",2023-11-22
"140","similar document template matching algorithm","harshitha yenigalla, bommareddy revanth srinivasa reddy, batta venkata rahul, nannapuraju hemanth raju","computer vision and pattern recognition","this study outlines a comprehensive methodology for verifying medical documents, integrating advanced techniques in template extraction, comparison, and fraud detection. it begins with template extraction using sophisticated region-of-interest (roi) methods, incorporating contour analysis and edge identification. pre-processing steps ensure template clarity through morphological operations and adaptive thresholding. the template comparison algorithm utilizes advanced feature matching with key points and descriptors, enhancing robustness through histogram-based analysis for accounting variations. fraud detection involves the ssim computation and ocr for textual information extraction. the ssim quantifies structural similarity, aiding in potential match identification. ocr focuses on critical areas like patient details, provider information, and billing amounts. extracted information is compared with a reference dataset, and confidence thresholding ensures reliable fraud detection. adaptive parameters enhance system flexibility for dynamic adjustments to varying document layouts. this methodology provides a robust approach to medical document verification, addressing complexities in template extraction, comparison, fraud detection, and adaptability to diverse document structures.",2023-11-21
"141","extracting definienda in mathematical scholarly articles with transformers","shufan jiang (valda), pierre senellart (di-ens, valda)","artificial intelligence","we consider automatically identifying the defined term within a mathematical definition from the text of an academic article. inspired by the development of transformer-based natural language processing applications, we pose the problem as (a) a token-level classification task using fine-tuned pre-trained transformers; and (b) a question-answering task using a generalist large language model (gpt). we also propose a rule-based approach to build a labeled dataset from the latex source of papers. experimental results show that it is possible to reach high levels of precision and recall using either recent (and expensive) gpt 4 or simpler pre-trained models fine-tuned on our task.",2023-11-21
"142","taiyi: a bilingual fine-tuned large language model for diverse biomedical tasks","ling luo, jinzhong ning, yingwen zhao, zhijun wang, zeyuan ding, peng chen, weiru fu, qinyu han, guangtao xu, yunzhi qiu, dinghao pan, jiru li, hao li, wenduo feng, senbo tu, yuqi liu, zhihao yang, jian wang, yuanyuan sun, hongfei lin","computation and language","objective: most existing fine-tuned biomedical large language models (llms) focus on enhancing performance in monolingual biomedical question answering and conversation tasks. to investigate the effectiveness of the fine-tuned llms on diverse biomedical nlp tasks in different languages, we present taiyi, a bilingual fine-tuned llm for diverse biomedical tasks. materials and methods: we first curated a comprehensive collection of 140 existing biomedical text mining datasets (102 english and 38 chinese datasets) across over 10 task types. subsequently, a two-stage strategy is proposed for supervised fine-tuning to optimize the model performance across varied tasks. results: experimental results on 13 test sets covering named entity recognition, relation extraction, text classification, question answering tasks demonstrate that taiyi achieves superior performance compared to general llms. the case study involving additional biomedical nlp tasks further shows taiyi's considerable potential for bilingual biomedical multi-tasking. conclusion: leveraging rich high-quality biomedical corpora and developing effective fine-tuning strategies can significantly improve the performance of llms within the biomedical domain. taiyi shows the bilingual multi-tasking capability through supervised fine-tuning. however, those tasks such as information extraction that are not generation tasks in nature remain challenging for llm-based generative approaches, and they still underperform the conventional discriminative approaches of smaller language models.",2023-11-20
"143","joint multifractal analysis of air temperature, relative humidity and reference evapotranspiration in the middle zone of the guadalquivir river valley","a.b. ariza-villaverde, p. pavon-dominguez, r. carmona-cabezas, e. gutierrez de rave, f.j. jimenez-hornero","atmospheric and oceanic physics","previous works have analysed the relationship existing between reference evapotranspiration (et0) and other climatic variables under a one-at-a-time perturbation condition. however, due to the physical relationships between these climatic variables is advisable to study their joint influence on et0. the box-counting joint multifractal algorithm describes the relations between variables using relevant information extracted from the data singularities. this work investigated the use of this algorithm to describe the simultaneous behaviour of et0, calculated by means of penman-monteith (pm) equation, and relative humidity (rh) and air temperature (t), influencing on it in the middle zone of the guadalquivir river valley, andalusia, southern spain. the studied cases were grouped according to the fractal dimension values, which were related to their probability of occurrence. the most likely cases were linked to smooth behaviour and weak dependence between variables, both circumstances were detected in the local multifractal analysis. for these cases, the rest of penman monteith (pm) equation variables, neither the t nor the rh, seemed to influence on et0 determination, especially when low t values were involved. by contrast, the least frequent cases were those with variables showing high fluctuations and strong relationship between them. in these situations, when t is low, the et0 is affected by the rest of pm equation variables. this fact confirmed t as main driver of et0 because the higher t values the lesser influence of other climate variables on et0. joint multifractal analysis shows some limitations when it is applied to large number of variables, the results reported are promising and suggest the convenience of exploring the relationships between et0 and other climatic variables not considered here with this framework such as wind speed and net radiation.",2023-11-18
"144","an attention-based denoising framework for personality detection in social media texts","qirui tang, wenkang jiang, yihua du, lei lin","computers and society","in social media networks, users produce a large amount of text content anytime, providing researchers with a valuable approach to digging for personality-related information. personality detection based on user-generated texts is a universal method that can be used to build user portraits. the presence of noise in social media texts hinders personality detection. however, previous studies have not fully addressed this challenge. inspired by the scanning reading technique, we propose an attention-based information extraction mechanism (aiem) for long texts, which is applied to quickly locate valuable pieces of information, and focus more attention on the deep semantics of key pieces. then, we provide a novel attention-based denoising framework (adf) for personality detection tasks and achieve state-of-the-art performance on two commonly used datasets. notably, we obtain an average accuracy improvement of 10.2% on the gold standard twitter-myers-briggs type indicator (twitter-mbti) dataset. we made our code publicly available on github. we shed light on how aiem works to magnify personality-related signals.",2023-11-16
"145","gsap-ner: a novel task, corpus, and baseline for scholarly entity extraction focused on machine learning models and datasets","wolfgang otto, matth√§us zloch, lu gan, saurav karmakar, stefan dietze","computation and language","named entity recognition (ner) models play a crucial role in various nlp tasks, including information extraction (ie) and text understanding. in academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. despite the advancements in ner, existing ground truth datasets do not treat fine-grained types like ml model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. in this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ml models and datasets. in order to provide a nuanced understanding of how ml models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like ""our bert-based model"" or ""an image cnn"". you can find the ground truth dataset and code to replicate model training at this https url.",2023-11-16
"146","care: extracting experimental findings from clinical literature","aakanksha naik, bailey kuehl, erin bransom, doug downey, tom hope","computation and language","extracting fine-grained experimental findings from literature can provide massive utility for scientific applications. prior work has focused on developing annotation schemas and datasets for limited aspects of this problem, leading to simpler information extraction datasets which do not capture the real-world complexity and nuance required for this task. focusing on biomedicine, this work presents care (clinical aggregation-oriented result extraction) -- a new ie dataset for the task of extracting clinical findings. we develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which includes phenomena challenging for current ie systems such as discontinuous entity spans, nested relations, and variable arity n-ary relations. using this schema, we collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. we also benchmark the performance of various state-of-the-art ie systems on our dataset, including extractive models and generative llms in fully supervised and limited data settings. our results demonstrate the difficulty of our dataset -- even sota models such as gpt4 struggle, particularly on relation extraction. we release our annotation schema and care to encourage further research on extracting and aggregating scientific findings from literature.",2023-11-16
"147","loke: linked open knowledge extraction for automated knowledge graph construction","jamie mccusker","computation and language","while the potential of open information extraction (open ie) for knowledge graph construction (kgc) may seem promising, we find that the alignment of open ie extraction results with existing knowledge graphs to be inadequate. the advent of large language models (llms), especially the commercially available openai models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. we investigate the use of gpt models and prompt engineering for knowledge graph construction with the wikidata knowledge graph to address a similar problem to open ie, which we call open knowledge extraction (oke) using an approach we call the linked open knowledge extractor (loke, pronounced like ""loki""). we consider the entity linking task essential to construction of real world knowledge graphs. we merge the carb benchmark scoring approach with data from the tekgen dataset for the loke task. we then show that a well engineered prompt, paired with a naive entity linking approach (which we call loke-gpt), outperforms allenai's openie 4 implementation on the oke task, although it over-generates triples compared to the reference set due to overall triple scarcity in the tekgen set. through an analysis of entity linkability in the carb dataset, as well as outputs from openie 4 and loke-gpt, we see that loke-gpt and the ""silver"" tekgen triples show that the task is significantly different in content from oie, if not structure. through this analysis and a qualitative analysis of sentence extractions via all methods, we found that loke-gpt extractions are of high utility for the kgc task and suitable for use in semi-automated extraction settings.",2023-11-15
"148","when does in-context learning fall short and why? a study on specification-heavy tasks","hao peng, xiaozhi wang, jianhui chen, weikai li, yunjia qi, zimu wang, zhili wu, kaisheng zeng, bin xu, lei hou, juanzi li","computation and language","in-context learning (icl) has become the default method for using large language models (llms), making the exploration of its limitations and understanding the underlying causes crucial. in this paper, we find that icl falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. the performance of icl on these tasks mostly cannot reach half of the state-of-the-art results. to explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various llms and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. furthermore, we demonstrate that through fine-tuning, llms can achieve decent performance on these tasks, indicating that the failure of icl is not an inherent flaw of llms, but rather a drawback of existing alignment methods that renders llms incapable of handling complicated specification-heavy tasks via icl. to substantiate this, we perform dedicated instruction tuning on llms for these tasks and observe a notable improvement. we hope the analyses in this paper could facilitate advancements in alignment methods enabling llms to meet more sophisticated human demands.",2023-11-15
"149","uncertainty estimation on sequential labeling via uncertainty transmission","jianfeng he, linlin yu, shuo lei, chang-tien lu, feng chen","computation and language","sequential labeling is a task predicting labels for each token in a sequence, such as named entity recognition (ner). ner tasks aim to extract entities and predict their labels given a text, which is important in information extraction. although previous works have shown great progress in improving ner performance, uncertainty estimation on ner (ue-ner) is still underexplored but essential. this work focuses on ue-ner, which aims to estimate uncertainty scores for the ner predictions. previous uncertainty estimation models often overlook two unique characteristics of ner: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. therefore, we propose a sequential labeling posterior network (slpn) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. our slpn has achieved significant improvements on two datasets, such as a 5.54-point improvement in aupr on the mit-restaurant dataset.",2023-11-15
"150","all data on the table: novel dataset and benchmark for cross-modality scientific information extraction","yuhan li, jian wu, zhiwei yu, b√∂rje f. karlsson, wei shen, manabu okumura, chin-yew lin","computation and language","extracting key information from scientific papers has the potential to help researchers work more efficiently and accelerate the pace of scientific progress. over the last few years, research on scientific information extraction (sciie) witnessed the release of several new systems and benchmarks. however, existing paper-focused datasets mostly focus only on specific parts of a manuscript (e.g., abstracts) and are single-modality (i.e., text- or table-only), due to complex processing and expensive annotations. moreover, core information can be present in either text or tables or across both. to close this gap in data availability and enable cross-modality ie, while alleviating labeling costs, we propose a semi-supervised pipeline for annotating entities in text, as well as entities and relations in tables, in an iterative procedure. based on this pipeline, we release novel resources for the scientific community, including a high-quality benchmark, a large-scale corpus, and a semi-supervised annotation pipeline. we further report the performance of state-of-the-art ie models on the proposed benchmark dataset, as a baseline. lastly, we explore the potential capability of large language models such as chatgpt for the current task. our new dataset, results, and analysis validate the effectiveness and efficiency of our semi-supervised pipeline, and we discuss its remaining limitations.",2023-11-14
